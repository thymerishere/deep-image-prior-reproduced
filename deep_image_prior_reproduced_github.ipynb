{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "deep-image-prior-reproduced-github.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thymerishere/deep-image-prior-reproduced/blob/master/deep_image_prior_reproduced_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdxzgRMOGWBo",
    "colab_type": "text"
   },
   "source": [
    "# Analysis of Deep Image Prior\n",
    "In this notebook we analyse different characteristics of several applications as described in the [original Deep Image Prior paper](https://arxiv.org/abs/1711.10925) published by Ulyanov et al.\n",
    "\n",
    "We looked at four different aspects:\n",
    "1. reproduce inpainting and analyse consequences of different settings of hyperparameters\n",
    "2. run the restoration/inpainting application code for new images with different settings of the hyperparameters to check which gives the best results\n",
    "3. test how the setup of skip connections in the network influence the performance of restoration logic on selected images\n",
    "4. reproduce table 1/figure 7 from the [original paper](https://arxiv.org/abs/1711.10925), also looking at how the reported value (PSNR) evolves over time\n",
    "\n",
    "Each of these tasks is contained in its own section below with separate description and code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNni_08IHbfN",
    "colab_type": "text"
   },
   "source": [
    "## How does Deep Image Prior work?\n",
    "With deep convolutional neural nets (ConvNets) being the state of the art in reverse image reconstruction, Ulyanov et al. [argue](https://arxiv.org/abs/1711.10925) that the assumption that the excellent performance is obtained due to the ability of ConvNets to learn realistic image priors from data, is flawed. They aim do demonstrate that image reconstruction can be done using only the information of the single degraded input image.\n",
    "This is done by interpreting the generator neural network as a parameterisation $x=f_\\theta(z)$ of an image $x \\in \\mathbb{R}^{3\\times H \\times W}$ with $z\\in \\mathbb{R}^{C' \\times H' \\times W'}$ a randomly initialised code vector. This then gives the following optimisation: $$\\theta^* = argmin_\\theta E(f_\\theta(z); x_0), \\quad x^*=f_{\\theta^*}(z)$$\n",
    "where, using gradient descent and starting from a random initialisation of the parameters, the aim is to achieve the optimal parameters $\\theta^*$ to obtain the optimal mapping of noise to output image $x^*=f_{\\theta^*}(z)$. The data term $E(x, x_0)$ is dependent on the task performed where Ulyanov et al discussed denoising, generic reconstruction (JPEG restoration), super-resolution, inpainting, restoration, natural pre-image and flash-no flash reconstruction.\n",
    "\n",
    "The statement from Ulyanov et al. is that along the path from random noise to the corrupted image the neural net will prefer repairing/following the natural structures in an image than the noisy structures. This is implemented in how the loss is computed. First the same corruption is applied to the output image before computing the loss against the target corrupted image. This means that the loss function does not explicitly drive the repair of the noisy structures. This is left over to the implicit behavior of the neural network. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G4qS11iIEoK",
    "colab_type": "text"
   },
   "source": [
    "## Experimental Setup\n",
    "The authors performed all tasks on images from the [GCF-BM3D dataset](http://www.cs.tut.fi/~foi/GCF-BM3D/) using varying hyperparameters which are explained in more depth in the [original paper's supplementary](https://dmitryulyanov.github.io/deep_image_prior). The code Ulyanov et al. have written to obtain their results in the original paper is [fully provided](https://github.com/DmitryUlyanov/deep-image-prior) and we use their code to obtain similar results on three of the tasks of the paper. We will also use a custom made set of images to view the performance on a different dataset. Next we will discuss how to setup the experiment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FREet7bqHV2N",
    "colab_type": "text"
   },
   "source": [
    "## Running this Experiment on Google Colab\n",
    "In order to run this experiment, a few steps need to be taken in order to load the files needed. First, get the [original Deep Image Prior project from github](https://github.com/DmitryUlyanov/deep-image-prior) and put it inside the root folder of your Google Drive home folder. Our code uses models and utility functions provided by the original authors, in addition certain tests/reproductions use the original dataset also.\n",
    "\n",
    "In some of our tests we use our own custom images. These need to be included in the data folder in your drive. To obtain these, download the provided extra dataset (the custom folder) from our github repository and put it inside `deep-image-prior-master/data`. This following code snippet will then mount your Google Drive and navigate to the project folder."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cy_5raUwFirA",
    "colab_type": "code",
    "outputId": "d99a963b-5766-43f7-a17f-cca40885535b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd drive/My\\ Drive/deep-image-prior-master"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/deep-image-prior-master\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxAG9-Ooc2Cl",
    "colab_type": "text"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRUwANrAKYOo",
    "colab_type": "text"
   },
   "source": [
    "## Inpainting\n",
    "Image inpainting is specified by Ulyanov et al. as reconstructing missing data given an image $x_0$ with missing pixels denoted by a binary mask $m \\in \\{0,1\\}^{H \\times W}$. This gives a data term of $$E(x;x_0)=||(x-x_0) \\odot m||^2$$ with $\\odot$ being Hadamard's product. This data term is then used in the optimisation objective discussed previously. The task is specified in the code supplied by Ulyanov et al. with a number of default hyper-parameters. We will discuss the reproduction process wherein we reproduce Figure 7 and 8 from the original paper and because no reasoning has been given on the chosen values for the hyper-parameters we will try to discuss some of them and their influence on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyCDMF1Sw0BT",
    "colab_type": "text"
   },
   "source": [
    "### Reproducing\n",
    "Using the following [code](https://github.com/DmitryUlyanov/deep-image-prior) from the original paper we can perform the inpainting tasks. Uncommenting either the location to the vase image and its mask or the Kate image and its mask replicates respectively the results from Figure 6 and 7 in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uN-atG0GH0VD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Imports\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from models.resnet import ResNet\n",
    "from models.unet import UNet\n",
    "from models.skip import skip\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "from utils.inpainting_utils import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "PLOT = True\n",
    "imsize = -1\n",
    "dim_div_by = 64"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_0Ty-sttH9Of",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "############### Select the correct image.\n",
    "\n",
    "# Uncomment for vase.\n",
    "img_path  = 'data/inpainting/vase.png'\n",
    "mask_path = 'data/inpainting/vase_mask.png'\n",
    "\n",
    "# Uncomment for Kate.\n",
    "# img_path  = 'data/inpainting/kate.png'\n",
    "# mask_path = 'data/inpainting/kate_mask.png'\n",
    "\n",
    "##############\n",
    "\n",
    "# Creating the image and image mask.\n",
    "img_pil, img_np = get_image(img_path, imsize)\n",
    "img_mask_pil, img_mask_np = get_image(mask_path, imsize)\n",
    "\n",
    "img_mask_pil = crop_image(img_mask_pil, dim_div_by)\n",
    "img_pil      = crop_image(img_pil,      dim_div_by)\n",
    "\n",
    "img_np      = pil_to_np(img_pil)\n",
    "img_mask_np = pil_to_np(img_mask_pil)\n",
    "\n",
    "# Visualising the task.\n",
    "img_mask_var = np_to_torch(img_mask_np).type(dtype)\n",
    "plot_image_grid([img_np, img_mask_np, img_mask_np*img_np], 3,11);"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v4ftQ5g5IB_J",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Setting hyperparameters. Automatically selects the hyperparameters\n",
    "# for the corresponding task.\n",
    "pad = 'reflection' # 'zero'\n",
    "OPT_OVER = 'net'\n",
    "OPTIMIZER = 'adam'\n",
    "\n",
    "if 'vase.png' in img_path:\n",
    "    INPUT = 'meshgrid'\n",
    "    input_depth = 2\n",
    "    LR = 0.01 \n",
    "    num_iter = 5001\n",
    "    param_noise = False\n",
    "    show_every = 50\n",
    "    figsize = 5\n",
    "    reg_noise_std = 0.03\n",
    "    \n",
    "    net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [128] * 5,\n",
    "               num_channels_up   = [128] * 5,\n",
    "               num_channels_skip = [0] * 5,  \n",
    "               upsample_mode='nearest', filter_skip_size=1, filter_size_up=3, filter_size_down=3,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "    \n",
    "elif ('kate.png' in img_path):\n",
    "    # Hyperparameters are the same as super resolution.\n",
    "    INPUT = 'noise'\n",
    "    input_depth = 32\n",
    "    LR = 0.01 \n",
    "    num_iter = 6001\n",
    "    param_noise = False\n",
    "    show_every = 50\n",
    "    figsize = 5\n",
    "    reg_noise_std = 0.03\n",
    "    \n",
    "    net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [128] * 5,\n",
    "               num_channels_up =   [128] * 5,\n",
    "               num_channels_skip =    [128] * 5,  \n",
    "               filter_size_up = 3, filter_size_down = 3, \n",
    "               upsample_mode='nearest', filter_skip_size=1,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "\n",
    "net = net.type(dtype)\n",
    "net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype)\n",
    "\n",
    "# Compute number of parameters\n",
    "s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
    "print ('Number of params: %d' % s)\n",
    "\n",
    "# Setting loss\n",
    "mse = torch.nn.MSELoss().type(dtype)\n",
    "\n",
    "img_var = np_to_torch(img_np).type(dtype)\n",
    "mask_var = np_to_torch(img_mask_np).type(dtype)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NkCcL1A7wwvk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Main loop.\n",
    "i = 0\n",
    "def closure():\n",
    "    \n",
    "    global i\n",
    "\n",
    "    if param_noise:\n",
    "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "            n = n + n.detach().clone().normal_() * n.std() / 50\n",
    "    \n",
    "    net_input = net_input_saved\n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "        \n",
    "        \n",
    "    out = net(net_input)\n",
    "   \n",
    "    total_loss = mse(out * mask_var, img_var * mask_var)\n",
    "    total_loss.backward()\n",
    "        \n",
    "    print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
    "    if  PLOT and i % show_every == 0:\n",
    "        print(\"Iteration \", i)\n",
    "        out_np = torch_to_np(out)\n",
    "        plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "\n",
    "p = get_params(OPT_OVER, net, net_input)\n",
    "optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
    "i = 0\n",
    "\n",
    "# Printing the result to the screen.\n",
    "\n",
    "out_np = torch_to_np(net(net_input))\n",
    "plot_image_grid([out_np], factor=5);"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI9e7egryw4o",
    "colab_type": "text"
   },
   "source": [
    "We can see that indeed we obtain a very similar results to the those presented in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2hiV6aY_5-t",
    "colab_type": "text"
   },
   "source": [
    "### Hyperparameter Analysis\n",
    "The [original paper](https://arxiv.org/abs/1711.10925) specifies, besides the network structure which will be discussed in the section on restoration, six hyperparameters:\n",
    "\n",
    "| Hyperparameter | Name in the code | Default value |\n",
    "| -------------- | ---------------- | ------------- |\n",
    "| Amount of input channels | `input_depth` | 2 for vase, 32 for Kate |\n",
    "| Network input type | `INPUT` | Mesh for vase, noise for Kate|\n",
    "| Standard deviation of the input noise $\\sigma_p$ | `reg_noise_std`| $\\frac{1}{20}$ |\n",
    "| Number of iterations | `num_iter` | 5000 |\n",
    "| Learning rate | `LR` | 0.1 |\n",
    "\n",
    "As the [original paper](https://arxiv.org/abs/1711.10925) states that its main aim is not to maximise PSNR, we will analyse the effect of the hyperparameters on the visual quality of the result on images from a dataset we created for this analysis. The following method allows us to run their code with different hyperparameters, images and masks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qutekcONS4NB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Defines inpainting method with default values.\n",
    "def inpainting(img_path, mask_path, INPUT='noise', input_depth=2, LR=0.01,\n",
    "               num_iter=5000, reg_noise_std=0.05):\n",
    "\n",
    "    # Creating the image and image mask.\n",
    "    img_pil, img_np = get_image(img_path, imsize)\n",
    "    img_mask_pil, img_mask_np = get_image(mask_path, imsize)\n",
    "\n",
    "    img_mask_pil = crop_image(img_mask_pil, dim_div_by)\n",
    "    img_pil      = crop_image(img_pil,      dim_div_by)\n",
    "\n",
    "    img_np      = pil_to_np(img_pil)\n",
    "    img_mask_np = pil_to_np(img_mask_pil)\n",
    "\n",
    "    # Visualising the task.\n",
    "    img_mask_var = np_to_torch(img_mask_np).type(dtype)\n",
    "    plot_image_grid([img_np, img_mask_np, img_mask_np*img_np], 3,11);\n",
    "\n",
    "    param_noise = False\n",
    "    show_every = 250\n",
    "    figsize = 5\n",
    "\n",
    "    pad = 'reflection' # 'zero'\n",
    "    OPT_OVER = 'net'\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    net = skip(input_depth, img_np.shape[0], \n",
    "                num_channels_down = [128] * 5,\n",
    "                num_channels_up =   [128] * 5,\n",
    "                num_channels_skip =    [128] * 5,  \n",
    "                filter_size_up = 3, filter_size_down = 3, \n",
    "                upsample_mode='nearest', filter_skip_size=1,\n",
    "                need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "\n",
    "    net = net.type(dtype)\n",
    "    net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype)\n",
    "\n",
    "    # Compute number of parameters\n",
    "    s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
    "    print ('Number of params: %d' % s)\n",
    "\n",
    "    # Setting loss\n",
    "    mse = torch.nn.MSELoss().type(dtype)\n",
    "\n",
    "    img_var = np_to_torch(img_np).type(dtype)\n",
    "    mask_var = np_to_torch(img_mask_np).type(dtype)\n",
    "\n",
    "    # Main loop.\n",
    "    global i\n",
    "    i = 0\n",
    "    def closure():\n",
    "        \n",
    "        global i\n",
    "\n",
    "        if param_noise:\n",
    "            for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "                n = n + n.detach().clone().normal_() * n.std() / 50\n",
    "        \n",
    "        net_input = net_input_saved\n",
    "        if reg_noise_std > 0:\n",
    "            net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "            \n",
    "            \n",
    "        out = net(net_input)\n",
    "    \n",
    "        total_loss = mse(out * mask_var, img_var * mask_var)\n",
    "        total_loss.backward()\n",
    "            \n",
    "        print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
    "        if  PLOT and i % show_every == 0:\n",
    "            print(\"Iteration \", i)\n",
    "            out_np = torch_to_np(out)\n",
    "            plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    net_input_saved = net_input.detach().clone()\n",
    "    noise = net_input.detach().clone()\n",
    "\n",
    "    p = get_params(OPT_OVER, net, net_input)\n",
    "    optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
    "\n",
    "    # Printing the result to the screen.\n",
    "\n",
    "    out_np = torch_to_np(net(net_input))\n",
    "    plot_image_grid([out_np], factor=5);"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCA7jdZA3C9c",
    "colab_type": "text"
   },
   "source": [
    "#### Number of iterations\n",
    "The number of iterations determines the amount of time steps the optimiser gets to optimise the over the objective function. The default amount of iterations is set to 5000. The [original paper](https://arxiv.org/abs/1711.10925) discusses the amount of iterations with regard to restoration and that eventualy the network will overfit to the image and thus reintroduce the artifacts of which it is the goal to remove. For inpainting it is not discussed what the repercussions of high number of iterations are. We expect that the image quality will improve up to a point where more learning adds no additional quality as it has learned on all known pixels. For this task we set the iterations to 15000 and determine when the image quality stops improving."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uv5uyBnY3Gsb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", num_iter=15000)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wvc0RpE3LWU",
    "colab_type": "text"
   },
   "source": [
    "It is clear that, as expected, after the first ~3000 iterations there is no significant improvement in image quality. It seems that the network resets a couple of times within the optimisation process which is explained by the [original paper's supplementary](https://dmitryulyanov.github.io/deep_image_prior): \"We found the optimization process tends to destabilize as the loss goes down and approaches a certain value. \\[...] To remedy this issue we simply track the optimization loss and return to parameters from the previous iteration if the loss difference between two consecutive iterations is higher than a certain threshold.\" After the resets the learning seems to bring the quality back to the point before the reset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFr7QJdE3ThS",
    "colab_type": "text"
   },
   "source": [
    "#### Learning rate\n",
    "The learning rate influences how much the network learns per iteration. The [original paper](https://arxiv.org/abs/1711.10925) states that the result is sensitive to the choice of learning rate as in the vase inpainting, with learning rate equal to $10^{-4}$, the location the mask was clearly visible. Therefore, for most tasks, a learning rate of $10^{-3}$ was chosen. In our example we expect to find a similar result where with too low learning rates the network will not be able to learn the known pixels in a reasonable time, while with too high learning rates the network will not be able to generate the known pixels accurately and always generate blurry images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VI6bROxw3Wzl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", LR=0.00001)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N5lSanSZ3Yh_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", LR=0.0001)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bFtxU6KE3aIZ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", LR=0.1)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5kxkDc23dN4",
    "colab_type": "text"
   },
   "source": [
    "Interstingly, with high learning rates, the network converges to a black image, while with very low learning rates the image gets increasingly blurrier, possible due to needing longer to converge. A low learning rate clearly blends the missing pixels better with the known pixels though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWSZmqLuTa2B",
    "colab_type": "text"
   },
   "source": [
    "#### Input depth\n",
    "The `input_depth` parameter changes the amount of input channels to the image. We would then expect the network to learn more features which would in turn lead to a better blend of the filled pixels as the network learned more of the masked image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5YpJUHhqT3uq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", input_depth=2)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xkluxwso3muv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", input_depth=32)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w-nA-7Uv--2",
    "colab_type": "text"
   },
   "source": [
    "From both runs we can conclude that, as expected, with a higher input depth the filled in pixels are less noisy and blend in better with the rest of the image. An interesting observation is that the number plate of the car is arguably more realistic when the amount of input channels is higher, as it is sharper, even though the filled in pixels do not represent any letters. Furthermore, the edges on the left headlamp are more defined with higher input depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6OHfyM6yM7G",
    "colab_type": "text"
   },
   "source": [
    "#### Network input type\n",
    "The network input type defines what the network receives as input from which it tries to reconstruct the masked image. The [original paper](https://arxiv.org/abs/1711.10925) presents two options: a mesh grid, which is used for the 'vase' example, and noise, which is used for the 'Kate' example. The mesh grid consists two channels of black to white gradients with one going left to right and one going top to bottom. The [original paper' supplementary material](https://dmitryulyanov.github.io/deep_image_prior) states that the mesh grid was deemed only useful when doing large-hole inpainting and not for other tasks. We expect this the case due to noise possibly being better at masking the edges of the mask. The following code snippet runs the inpainting task on the 'car' image with both input types."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "54zc_HDy3jsF",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", INPUT='meshgrid')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dOp7BKJc8alq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", INPUT='noise')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPYwMw9V-neE",
    "colab_type": "text"
   },
   "source": [
    "It is clear from the results that with the traces of the mask are more defined when the mesh is used instead of noise, as expected. Also, when using noise the image is sharper and more defined than when using the mesh grid as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxTT1ujk3y-o",
    "colab_type": "text"
   },
   "source": [
    "#### Input noise standard deviation\n",
    "When `reg_noise_std` is greater than 0, at each iteration additional normal noise with a factor of `reg_noise_std` is added to the input of the network. The [original paper's supplementary](https://dmitryulyanov.github.io/deep_image_prior) notes that this is used as a regularisation which impedes the optimisation process, but that the network will always optimise the objective to 0. We are more interested in the visual effect of this regularisation technique and the following code snippets perform the inpainting with different values for `reg_noise_std`. We expect the additional noise to make the network more robust to noise and therefore to create a smoother image which better blends the borders between the known and unknown pixels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VDpllKW_4Fid",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", reg_noise_std=0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dwwGl1Tf4GJo",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", reg_noise_std=0.1)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SdhMaSmP4JLq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", reg_noise_std=0.3)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHlz7NPP4NEY",
    "colab_type": "text"
   },
   "source": [
    "It is clear that when no added input noise is used, the mask is clearly visible which returns a visually very unappealing image, but when the added noise is too much the optimiser has difficulties even optimising over the known pixels. A value of 0.1 indicates that it is best to have something in between, which is to be expected as [too much regularasation makes the data lose important properties](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK5TBYewvd5V",
    "colab_type": "text"
   },
   "source": [
    "### Other images\n",
    "The following code snippets will run the network on other images from the custom dataset. The first image is of a large group of people with a larger mask which requires a large amount of detailed inpainting. We will try two different approaches. First we will use noise with a low learning rate, high added input noise and high input depth to try and create a smooth image. For the second approach we will try and replicate the busy environment with a high learning rate and less smoothing and lower input depth."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9AFJybECvyA-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/people2.png\", \"data/custom/mask2.png\", reg_noise_std=0.2, LR=0.001, input_depth=32, INPUT='noise')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "trbg2stSWuYA",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/people2.png\", \"data/custom/mask2.png\", reg_noise_std=0.05, LR=0.1, input_depth=2, INPUT='noise')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL5iGW72WgI5",
    "colab_type": "text"
   },
   "source": [
    "The results of both runs really vary only in the way they painted in the missing pixels. The first run has more blurry pixels while the second run is has more noisy pixels. This blurriness let us decide on the second result being more realistic.\n",
    "\n",
    "The next picture consists of a grass field with a sky with clouds. We will again use the same to approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3ydhKKCl4zwt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/grass.png\", \"data/custom/mask2.png\", reg_noise_std=0.2, LR=0.001, input_depth=32, INPUT='noise')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BEjPTHHA40L9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "inpainting(\"data/custom/grass.png\", \"data/custom/mask2.png\", reg_noise_std=0.05, LR=0.1, input_depth=2, INPUT='noise')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qob65ng4R2gw",
    "colab_type": "text"
   },
   "source": [
    "It is clear that the first set of hyperparameters achieve a better result. It is very hard to determine where the mask has been. The second set of hyperparameters makes the learning very unstable which leads to a very blurry image, or no result at all where only a black image is generated. This is probably due to the learning rate being too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSYj5Si_KaN5",
    "colab_type": "text"
   },
   "source": [
    "## Determining the influence of different Skip connection settings\n",
    "\n",
    "The restoration NN used in the Deep Image Prior paper has a U-Net form with skip connections. The skip connections are assumed to influence the performance in both result as well as speed of learning. This code in this section will try to analyse this influence by testing different network layouts with less or no skip connections. The test is done on two images (barbara.png and man.png) to make it more feasible to compare with the results of the standard restoration by Deep Image Prior.\n",
    "\n",
    "The code below is mostly copied from the [original restoration](https://github.com/DmitryUlyanov/deep-image-prior/blob/master/restoration.ipynb) ipynb, changes have been made to allow running with different skip connection setups. Changes/additions are noted with DL.added.\n",
    "\n",
    "The logic is combined in one function which depending on its parameter executes the run for the following network structures:\n",
    "- original (with skip connections which have conv, batchnorm and activation)\n",
    "- identity skip connections for all the down-up steps\n",
    "- no skip connections\n",
    "- unet\n",
    "\n",
    "The psnr, masked psnr and loss are captured for each iteration step and printed at the end as a json array. The psrn is plotted using pyplot.\n",
    "\n",
    "The script can be run for different figures, uncomment the relevant line: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bswcm4L76tLO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# uncomment the image you want to test. Note this code assumes that the images are located in the \n",
    "# data/restoration folder inside your google drive.\n",
    "#f = './data/restoration/barbara.png'\n",
    "#f = './data/restoration/man.png'\n",
    "#f = './data/restoration/montage.png'\n",
    "#f = './data/restoration/hill.png'\n",
    "#f = './data/restoration/boat.png'\n",
    "#f = './data/restoration/house.png'\n",
    "#f = './data/restoration/Lena512.png'\n",
    "f = './data/restoration/Cameraman256.png'\n",
    "#f = './data/restoration/peppers256.png'\n",
    "#f = './data/restoration/fingerprint.png'\n",
    "#f = './data/restoration/couple.png'"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pT4W_gNmYroh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from models.resnet import ResNet\n",
    "from models.unet import UNet\n",
    "from models.skip import skip\n",
    "from models import get_net\n",
    "import torch\n",
    "import torch.optim\n",
    "from skimage.measure import compare_psnr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.common import *\n",
    "\n",
    "from utils.inpainting_utils import *"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GhF9XsEhYtdL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def runSkipTest(scenario='original'):\n",
    "\n",
    "  global i, psrn_masked_last, last_net, net_input, psrns, psrns_masked, max_iter, max_psrn, losses\n",
    "\n",
    "  def closure():\n",
    "\n",
    "    global i, psrn_masked_last, last_net, net_input, psrns, psrns_masked, max_iter, max_psrn\n",
    "    \n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "    \n",
    "    out = net(net_input)\n",
    "\n",
    "    total_loss = mse(out * mask_var, img_var * mask_var)\n",
    "    total_loss.backward()\n",
    "    losses.append(total_loss.item())\n",
    "    \n",
    "    # print(img_masked.shape)\n",
    "    # print((out.detach().cpu().numpy()[0] * img_mask_np).shape)\n",
    "    psrn_masked = compare_psnr(img_masked, out.detach().cpu().numpy()[0] * img_mask_np) \n",
    "    comp_psrn_masked = compare_psnr(img_masked, out.detach().cpu().numpy()[0]) \n",
    "    psrn = compare_psnr(img_np, out.detach().cpu().numpy()[0]) \n",
    "\n",
    "    #DL.added\n",
    "    psrns.append(psrn)\n",
    "    psrns_masked.append(comp_psrn_masked)\n",
    "    if psrn > max_psrn:\n",
    "      max_psrn = psrn\n",
    "      max_iter = i    \n",
    "    #DL.added\n",
    "\n",
    "    if  PLOT and i % show_every == 0:\n",
    "        out_np = torch_to_np(out)\n",
    "        \n",
    "        # Backtracking\n",
    "        if psrn_masked - psrn_masked_last < -5: \n",
    "            print('Falling back to previous checkpoint.')\n",
    "\n",
    "            for new_param, net_param in zip(last_net, net.parameters()):\n",
    "                net_param.data.copy_(new_param.cuda())\n",
    "\n",
    "            return total_loss*0\n",
    "        else:\n",
    "            last_net = [x.cpu() for x in net.parameters()]\n",
    "            psrn_masked_last = psrn_masked\n",
    "\n",
    "        print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psrn))  #DL.added\n",
    "\n",
    "        plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "  def mySkip(\n",
    "      num_input_channels=2, num_output_channels=3, skip_type='original',\n",
    "      num_channels_down=[16, 32, 64, 128, 128], num_channels_up=[16, 32, 64, 128, 128], num_channels_skip=[4, 4, 4, 4, 4], \n",
    "      filter_size_down=3, filter_size_up=3, filter_skip_size=1,\n",
    "      need_sigmoid=True, need_bias=True, \n",
    "      pad='zero', upsample_mode='nearest', downsample_mode='stride', act_fun='LeakyReLU', \n",
    "      need1x1_up=True):\n",
    "    \"\"\"Assembles encoder-decoder with skip connections.\n",
    "\n",
    "    Arguments:\n",
    "        act_fun: Either string 'LeakyReLU|Swish|ELU|none' or module (e.g. nn.ReLU)\n",
    "        pad (string): zero|reflection (default: 'zero')\n",
    "        upsample_mode (string): 'nearest|bilinear' (default: 'nearest')\n",
    "        downsample_mode (string): 'stride|avg|max|lanczos2' (default: 'stride')\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n",
    "\n",
    "    n_scales = len(num_channels_down) \n",
    "\n",
    "    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)) :\n",
    "      upsample_mode   = [upsample_mode]*n_scales\n",
    "\n",
    "    if not (isinstance(downsample_mode, list)or isinstance(downsample_mode, tuple)):\n",
    "      downsample_mode   = [downsample_mode]*n_scales\n",
    "    \n",
    "    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)) :\n",
    "      filter_size_down   = [filter_size_down]*n_scales\n",
    "\n",
    "    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)) :\n",
    "      filter_size_up   = [filter_size_up]*n_scales\n",
    "\n",
    "    last_scale = n_scales - 1 \n",
    "\n",
    "    cur_depth = None\n",
    "\n",
    "    model = nn.Sequential()\n",
    "    model_tmp = model\n",
    "\n",
    "    input_depth = num_input_channels\n",
    "    for i in range(len(num_channels_down)):\n",
    "\n",
    "      deeper = nn.Sequential()\n",
    "      skip = nn.Sequential()\n",
    "\n",
    "      if num_channels_skip[i] != 0:\n",
    "        model_tmp.add(Concat(1, skip, deeper))\n",
    "      else:\n",
    "        model_tmp.add(deeper)\n",
    "      \n",
    "      model_tmp.add(bn(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i])))\n",
    "\n",
    "      if num_channels_skip[i] != 0:\n",
    "        if skip_type == 'original': #DL.added\n",
    "          skip.add(conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n",
    "          skip.add(bn(num_channels_skip[i]))\n",
    "          skip.add(act(act_fun))\n",
    "        else:\n",
    "          pass\n",
    "            \n",
    "      deeper.add(conv(input_depth, num_channels_down[i], filter_size_down[i], 2, bias=need_bias, pad=pad, downsample_mode=downsample_mode[i]))\n",
    "      deeper.add(bn(num_channels_down[i]))\n",
    "      deeper.add(act(act_fun))\n",
    "\n",
    "      deeper.add(conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n",
    "      deeper.add(bn(num_channels_down[i]))\n",
    "      deeper.add(act(act_fun))\n",
    "\n",
    "      deeper_main = nn.Sequential()\n",
    "\n",
    "      if i == len(num_channels_down) - 1:\n",
    "        # The deepest\n",
    "        k = num_channels_down[i]\n",
    "      else:\n",
    "        deeper.add(deeper_main)\n",
    "        k = num_channels_up[i + 1]\n",
    "\n",
    "      deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i]))\n",
    "\n",
    "      model_tmp.add(conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad))\n",
    "      model_tmp.add(bn(num_channels_up[i]))\n",
    "      model_tmp.add(act(act_fun))\n",
    "\n",
    "      if need1x1_up:\n",
    "        model_tmp.add(conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n",
    "        model_tmp.add(bn(num_channels_up[i]))\n",
    "        model_tmp.add(act(act_fun))\n",
    "\n",
    "      input_depth = num_channels_down[i]\n",
    "      model_tmp = deeper_main\n",
    "\n",
    "    model.add(conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n",
    "    if need_sigmoid:\n",
    "      model.add(nn.Sigmoid())\n",
    "\n",
    "    return model\n",
    "\n",
    "  torch.backends.cudnn.enabled = True\n",
    "  torch.backends.cudnn.benchmark =True\n",
    "  dtype = torch.cuda.FloatTensor\n",
    "\n",
    "  PLOT = True\n",
    "  imsize=-1\n",
    "  dim_div_by = 64\n",
    "  dtype = torch.cuda.FloatTensor\n",
    "\n",
    "  img_pil, img_np = get_image(f, imsize)\n",
    "\n",
    "  if scenario == 'original' or scenario == 'original_myskip':  #DL.added\n",
    "    img_np = nn.ReflectionPad2d(1)(np_to_torch(img_np))[0].numpy()\n",
    "  \n",
    "  img_pil = np_to_pil(img_np)\n",
    "\n",
    "  img_mask = get_bernoulli_mask(img_pil, 0.50)\n",
    "  img_mask_np = pil_to_np(img_mask)\n",
    "\n",
    "  img_masked = img_np * img_mask_np\n",
    "\n",
    "  mask_var = np_to_torch(img_mask_np).type(dtype)\n",
    "\n",
    "  plot_image_grid([img_np, img_mask_np, img_mask_np * img_np], 3,11);\n",
    "\n",
    "  show_every=500\n",
    "\n",
    "  figsize=5\n",
    "  pad = 'reflection' # 'zero'\n",
    "  INPUT = 'noise'\n",
    "  input_depth = 32\n",
    "  OPT_OVER =  'net'\n",
    "  OPTIMIZER = 'adam'\n",
    "\n",
    "  LR = 0.001\n",
    "  num_iter = 11001\n",
    "  reg_noise_std = 0.03\n",
    "\n",
    "  NET_TYPE = 'skip'\n",
    "  #DL.added\n",
    "  if scenario == 'original':\n",
    "    # the original method of creating the net for restoration\n",
    "    net = get_net(input_depth, 'skip', pad, n_channels=1,\n",
    "                  skip_n33d=128, \n",
    "                  skip_n33u=128, \n",
    "                  skip_n11=4, \n",
    "                  num_scales=5,\n",
    "                  upsample_mode='bilinear').type(dtype)\n",
    "  elif scenario == 'unet':\n",
    "    # unet implementation\n",
    "    net = UNet(num_input_channels=input_depth, num_output_channels=1, \n",
    "                   feature_scale=4, more_layers=0, concat_x=False,\n",
    "                   upsample_mode='bilinear', pad=pad, norm_layer=nn.BatchNorm2d, need_sigmoid=True, need_bias=True)\n",
    "  elif scenario == 'original_myskip':\n",
    "    # the same as the original only now using the mySkip function\n",
    "    net = mySkip(input_depth, 1, \n",
    "                num_channels_down = [128, 128, 128, 128, 128],\n",
    "                num_channels_up   = [128, 128, 128, 128, 128],\n",
    "                num_channels_skip =    [4, 4, 4, 4, 4],   \n",
    "                upsample_mode='bilinear', \n",
    "                downsample_mode='stride', act_fun='LeakyReLU',\n",
    "                need_sigmoid=True, need_bias=True, pad=pad).type(dtype)\n",
    "  elif scenario == 'identity_skip':\n",
    "    # network with skip connections, but skip connections are identity\n",
    "    # connections without batchnorm etc.\n",
    "    net = mySkip(input_depth, 1, skip_type='identity', \n",
    "                num_channels_down = [128, 128, 128, 128, 128],\n",
    "                num_channels_up   = [128, 128, 128, 128, 128],\n",
    "                num_channels_skip =    [32, 128, 128, 128, 128],   \n",
    "                upsample_mode='bilinear', \n",
    "                downsample_mode='stride', act_fun='LeakyReLU',\n",
    "                need_sigmoid=True, need_bias=True, pad=pad).type(dtype)\n",
    "  elif scenario == 'no_skip':\n",
    "    # network without skip connections\n",
    "    net = mySkip(input_depth, 1, \n",
    "                num_channels_down = [128, 128, 128, 128, 128],\n",
    "                num_channels_up   = [128, 128, 128, 128, 128],\n",
    "                num_channels_skip = [0, 0, 0, 0, 0],   \n",
    "                upsample_mode='bilinear', \n",
    "                downsample_mode='stride', act_fun='LeakyReLU',\n",
    "                need_sigmoid=True, need_bias=True, pad=pad).type(dtype)\n",
    "  else: \n",
    "    assert False\n",
    "  #DL.added\n",
    "\n",
    "  net = net.type(dtype)\n",
    "\n",
    "  # print the network structure\n",
    "  #DL.added\n",
    "  from torchsummary import summary\n",
    "  summary(net, input_size=(32, img_np.shape[1], img_np.shape[2]))\n",
    "  #DL.added\n",
    "\n",
    "  # Loss\n",
    "  mse = torch.nn.MSELoss().type(dtype)\n",
    "  img_var = np_to_torch(img_np).type(dtype)\n",
    "\n",
    "  net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype).detach()\n",
    "\n",
    "  # Init globals \n",
    "  last_net = None\n",
    "  psrn_masked_last = 0\n",
    "  i = 0\n",
    "\n",
    "  net_input_saved = net_input.detach().clone()\n",
    "  noise = net_input.detach().clone()\n",
    "\n",
    "  max_psrn = -1\n",
    "  max_iter = -1\n",
    "  psrns = []\n",
    "  psrns_masked = []\n",
    "  losses = []\n",
    "\n",
    "  # Run\n",
    "  p = get_params(OPT_OVER, net, net_input)\n",
    "  optimize(OPTIMIZER, p, closure, LR=LR, num_iter=num_iter)\n",
    "\n",
    "  #DL.added\n",
    "  print(max_psrn)\n",
    "  print(max_iter)\n",
    "  print('psrns')\n",
    "  print(psrns)\n",
    "  print('psrns_masked')\n",
    "  print(psrns_masked)\n",
    "  print('losses')\n",
    "  print(losses)\n",
    "\n",
    "  plt.plot(psrns)\n",
    "  plt.xticks(range(0, 11001, 500))\n",
    "  plt.ylabel('psnr')\n",
    "  plt.xlabel('iterations')\n",
    "  plt.grid()\n",
    "  plt.show()\n",
    "  #DL.added\n",
    "\n",
    "  out_np = torch_to_np(net(net_input))\n",
    "  q = plot_image_grid([np.clip(out_np, 0, 1), img_np], factor=13);\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGuJSIDs2-vE",
    "colab_type": "text"
   },
   "source": [
    "### Run with Original configuration\n",
    "Run the original network, for testing it is possible to run the original utility method or the adapted copy. The resulting network is the same."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4-6LryCQ3N9Y",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "runSkipTest(scenario='original')\n",
    "#runSkipTest(scenario='original_myskip')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdWW534F3VE6",
    "colab_type": "text"
   },
   "source": [
    "### Identity Skip connections\n",
    "In this network structure the skip connections are still there but are implemented as identity steps. So without convolution, batchnorm or activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fUPWCf4g3oM4",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "runSkipTest(scenario='identity_skip')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWbuAg1X3qaR",
    "colab_type": "text"
   },
   "source": [
    "### No Skip connections\n",
    "In this network structure there are no skip connections. Making it very similar to a UNet, but then using the exact configuration for the down and up layers as in the original network.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aWpfl8Nd4Mmr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "runSkipTest(scenario='no_skip')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuV6k83a349p",
    "colab_type": "text"
   },
   "source": [
    "### Unet\n",
    "For try-test purposes it is also possible to run the restoration using the UNet network provided by the authors. This network is used in the article to compare results if only a 2% of the pixels remain. Here we test with 50% remaining pixels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ojHB5VDDYz74",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "runSkipTest(scenario='unet')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qchzft9D4Mgm",
    "colab_type": "text"
   },
   "source": [
    "## Reproducing Table 1 - Figure 7 (bottom)\n",
    "\n",
    "The [Deep Image Prior paper](https://arxiv.org/abs/1711.10925) compares its method of restoring images corrupted with a 50% noise mask with an approach using convolutional sparse coding [1]. The results are presented in table 1 of the Deep Image Prior article.  The claim made by the authors is that the deep image prior performs better than the approach of [1]. In this section we will try to reproduce the deep-image-prior results. \n",
    "\n",
    "Table 1 in the [Deep Image Prior paper](https://arxiv.org/abs/1711.10925) displays PSNR (Peak Signal to Noise Ratio) values. The question which comes up is how these PSNR values are determined. This because the Deep Image Prior approach means that during training of the neural network different PSNR values are obtained and PSNR values can vary up and down during the iterations. \n",
    "\n",
    "In addition the code in the paper includes a fall back step. If the network deviates too strongly in the wrong direction it will fall back to a previous point and restart the training from there. \n",
    "\n",
    "To get a feel for both the development of the PSNR and the fallback consequences we keep track and then plot the PSNR values for each iteration.\n",
    "\n",
    "The code below is mostly copied from the original [restoration](https://github.com/DmitryUlyanov/deep-image-prior/blob/master/restoration.ipynb) ipynb, with extra code added to load the images, place all in one cell in this notebook and plot the PSNR values.\n",
    "\n",
    "To run the code, uncomment the line with the image you want to try, and run the cells in this section one by one, with the last cell being the call to the reproduce method.\n",
    "\n",
    "[1] V. Papyan, Y. Romano, J. Sulam, and M. Elad. Convolutional dictionary learning via local processing. In Proc.\n",
    "ICCV, 2017.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g13XbambXe5f",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# uncomment the image you want to test. Note this code assumes that the images are located in the \n",
    "# data/restoration folder inside your google drive.\n",
    "#f = './data/restoration/barbara.png'\n",
    "f = './data/restoration/man.png'\n",
    "#f = './data/restoration/montage.png'\n",
    "#f = './data/restoration/hill.png'\n",
    "#f = './data/restoration/boat.png'\n",
    "#f = './data/restoration/house.png'\n",
    "#f = './data/restoration/Lena512.png'\n",
    "#f = './data/restoration/Cameraman256.png'\n",
    "#f = './data/restoration/peppers256.png'\n",
    "#f = './data/restoration/fingerprint.png'\n",
    "#f = './data/restoration/couple.png'"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KJzWauqI7LKX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# reproducing table 1\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from utils.inpainting_utils import *\n",
    "\n",
    "# Init\n",
    "psrns = []\n",
    "psrns_masked = []\n",
    "max_psrn = 0\n",
    "max_iter = 0\n",
    "\n",
    "last_net = None\n",
    "psrn_masked_last = 0\n",
    "i = 0\n",
    "\n",
    "def reproduce():\n",
    "\n",
    "  #os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "  from models.resnet import ResNet\n",
    "  from models.unet import UNet\n",
    "  from models.skip import skip\n",
    "  from models import get_net\n",
    "  from skimage.measure import compare_psnr\n",
    "\n",
    "  torch.backends.cudnn.enabled = True\n",
    "  torch.backends.cudnn.benchmark =True\n",
    "  dtype = torch.cuda.FloatTensor\n",
    "\n",
    "  PLOT = True\n",
    "  imsize=-1\n",
    "  dim_div_by = 64\n",
    "  dtype = torch.cuda.FloatTensor\n",
    "\n",
    "  img_pil, img_np = get_image(f, imsize)\n",
    "\n",
    "  img_np = nn.ReflectionPad2d(1)(np_to_torch(img_np))[0].numpy()\n",
    "  img_pil = np_to_pil(img_np)\n",
    "  \n",
    "  img_mask = get_bernoulli_mask(img_pil, 0.50)\n",
    "  img_mask_np = pil_to_np(img_mask)\n",
    "\n",
    "  img_masked = img_np * img_mask_np\n",
    "\n",
    "  mask_var = np_to_torch(img_mask_np).type(dtype)\n",
    "\n",
    "  plot_image_grid([img_np, img_mask_np, img_mask_np * img_np], 3,11);\n",
    "\n",
    "  show_every=500\n",
    "  figsize=5\n",
    "  pad = 'reflection' # 'zero'\n",
    "  INPUT = 'noise'\n",
    "  input_depth = 32\n",
    "  OPTIMIZER = 'adam'\n",
    "  OPT_OVER =  'net'\n",
    "  \n",
    "  LR = 0.001\n",
    "  num_iter = 11001\n",
    "  reg_noise_std = 0.03\n",
    "  \n",
    "  NET_TYPE = 'skip'\n",
    "  net = get_net(input_depth, 'skip', pad, n_channels=1,\n",
    "                skip_n33d=128, \n",
    "                skip_n33u=128, \n",
    "                skip_n11=4, \n",
    "                num_scales=5,\n",
    "                upsample_mode='bilinear').type(dtype)\n",
    "      \n",
    "  # Loss\n",
    "  mse = torch.nn.MSELoss().type(dtype)\n",
    "  img_var = np_to_torch(img_np).type(dtype)\n",
    "\n",
    "  net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype).detach()\n",
    "\n",
    "  def closure():\n",
    "\n",
    "    global i, psrn_masked_last, last_net, net_input, max_psrn, max_iter, psrns, psrns_masked\n",
    "    \n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "    \n",
    "    out = net(net_input)\n",
    "\n",
    "    total_loss = mse(out * mask_var, img_var * mask_var)\n",
    "    total_loss.backward()\n",
    "    \n",
    "    psrn_masked = compare_psnr(img_masked, out.detach().cpu().numpy()[0] * img_mask_np) \n",
    "    psrn = compare_psnr(img_np, out.detach().cpu().numpy()[0]) \n",
    "\n",
    "    psrns.append(psrn)\n",
    "    psrns_masked.append(psrn_masked)\n",
    "    if psrn > max_psrn:\n",
    "      max_psrn = psrn\n",
    "      max_iter = i\n",
    "    print ('Iteration %05d    Loss %f PSNR_masked %f PSNR %f' % (i, total_loss.item(), psrn_masked, psrn),'\\r', end='')\n",
    "    \n",
    "    \n",
    "    if  PLOT and i % show_every == 0:\n",
    "        out_np = torch_to_np(out)\n",
    "        \n",
    "        # Backtracking\n",
    "        if psrn_masked - psrn_masked_last < -5: \n",
    "            print('Falling back to previous checkpoint.')\n",
    "\n",
    "            for new_param, net_param in zip(last_net, net.parameters()):\n",
    "                net_param.data.copy_(new_param.cuda())\n",
    "\n",
    "            return total_loss*0\n",
    "        else:\n",
    "            last_net = [x.cpu() for x in net.parameters()]\n",
    "            psrn_masked_last = psrn_masked\n",
    "\n",
    "\n",
    "        print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psrn))\n",
    "\n",
    "        plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "  net_input_saved = net_input.detach().clone()\n",
    "  noise = net_input.detach().clone()\n",
    "\n",
    "  # Run\n",
    "  p = get_params(OPT_OVER, net, net_input)\n",
    "  optimize(OPTIMIZER, p, closure, LR=LR, num_iter=num_iter)\n",
    "\n",
    "  print(max_psrn)\n",
    "  print(max_iter)\n",
    "  out_np = torch_to_np(net(net_input))\n",
    "  q = plot_image_grid([np.clip(out_np, 0, 1), img_np], factor=13);\n",
    "\n",
    "  plt.plot(psrns)\n",
    "  plt.xticks(range(0, 11001, 500))\n",
    "  plt.ylabel('psnr')\n",
    "  plt.xlabel('iterations')\n",
    "  plt.grid()\n",
    "  plt.show()\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JYInO3OO98bd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "reproduce()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJxiO3JQKcVy",
    "colab_type": "text"
   },
   "source": [
    "## Super Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBALp_051KzO",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "### Reproduction\n",
    "\n",
    "Another application of deep image priors discussed in the paper is super resolution. The aim of this task is to create a high resolution image from a low resolution image by upsampling it with a factor $t$. In other words, when having a low resolution image $x_0 \\in ℝ^{3 \\times H \\times W}$, you generate the high resolution image $x \\in ℝ^{3 \\times tH \\times tW}$. To solve this problem the data term is set to $E(x;x_0) = ||d(x) - x_0||^2$, where $d(\\cdot) : ℝ^{3 \\times tH \\times tW} \\to ℝ^{3 \\times H \\times W}$ is a downsampling operator that resizes the high resolution image to a low resolution image by a factor $t$. The goal is to minimize $E$ and therefore to find a high resolution image, when downsampling is the same as the original low resolution image. \n",
    "\n",
    "The paper evaluated two datasets and showed the results in the supplementary material. We will reproduce one dataset since we have limited time and the differences in images between the datasets are not very significant in our opinion. It would be more interesting to adjust the code to get better results. The code for reproduction is given below and to run the code, uncomment the line of the image you want to try and the factor of how much you want to upscale the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xYbydS1JqmEI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path_to_image = 'custom/Set5/baby.png'\n",
    "# path_to_image = 'custom/Set5/bird.png'\n",
    "# path_to_image = 'custom/Set5/butterfly.png'\n",
    "# path_to_image = 'custom/Set5/head.png'\n",
    "# path_to_image = 'custom/Set5/woman.png'\n",
    "\n",
    "factor = 4\n",
    "# factor = 8 "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    "The following code should be run before the running the super resolution replication code and the super resolution hyper parameters analysis code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NA_v2G7vqKm4",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from models import *\n",
    "import torch\n",
    "import torch.optim\n",
    "from skimage.measure import compare_psnr\n",
    "from models.downsampler import Downsampler\n",
    "from utils.sr_utils import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "imsize = -1 \n",
    "enforse_div32 = 'CROP' # we usually need the dimensions to be divisible by a power of two (32 in this case)\n",
    "PLOT = True"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reproduce super resolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nWj7O0SCbIHc",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Starts here\n",
    "imgs = load_LR_HR_imgs_sr(path_to_image , imsize, factor, enforse_div32)\n",
    "\n",
    "\n",
    "input_depth = 32\n",
    " \n",
    "INPUT =     'noise'\n",
    "pad   =     'reflection'\n",
    "OPT_OVER =  'net'\n",
    "KERNEL_TYPE='lanczos2'\n",
    "\n",
    "LR = 0.01\n",
    "tv_weight = 0.0\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "\n",
    "if factor == 4: \n",
    "    num_iter = 2000\n",
    "    reg_noise_std = 0.03\n",
    "elif factor == 8:\n",
    "    num_iter = 4000\n",
    "    reg_noise_std = 0.05\n",
    "else:\n",
    "    assert False, 'We did not experiment with other factors'\n",
    "\n",
    "\n",
    "net_input = get_noise(input_depth, INPUT, (imgs['HR_pil'].size[1], imgs['HR_pil'].size[0])).type(dtype).detach()\n",
    "\n",
    "NET_TYPE = 'skip' # UNet, ResNet\n",
    "net = get_net(input_depth, 'skip', pad,\n",
    "              skip_n33d=128, \n",
    "              skip_n33u=128, \n",
    "              skip_n11=4, \n",
    "              num_scales=5,\n",
    "              upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "# Losses\n",
    "mse = torch.nn.MSELoss().type(dtype)\n",
    "\n",
    "img_LR_var = np_to_torch(imgs['LR_np']).type(dtype)\n",
    "\n",
    "downsampler = Downsampler(n_planes=3, factor=factor, kernel_type=KERNEL_TYPE, phase=0.5, preserve_size=True).type(dtype)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    global i, net_input, psnr_best_HR, best_image, best_iteration\n",
    "    \n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "\n",
    "    out_HR = net(net_input)\n",
    "    out_LR = downsampler(out_HR)\n",
    "\n",
    "    total_loss = mse(out_LR, img_LR_var) \n",
    "    \n",
    "    if tv_weight > 0:\n",
    "        total_loss += tv_weight * tv_loss(out_HR)\n",
    "        \n",
    "    total_loss.backward()\n",
    "\n",
    "    # Log\n",
    "    psnr_LR = compare_psnr(imgs['LR_np'], torch_to_np(out_LR))\n",
    "    psnr_HR = compare_psnr(imgs['HR_np'], torch_to_np(out_HR))\n",
    "                      \n",
    "    # History\n",
    "    psnr_history.append([psnr_LR, psnr_HR])\n",
    "    \n",
    "    if psnr_HR > psnr_best_HR:\n",
    "        psnr_best_HR = psnr_HR\n",
    "        best_image = out_HR\n",
    "        best_iteration = i\n",
    "\n",
    "    if PLOT and i % 100 == 0:\n",
    "        out_HR_np = torch_to_np(out_HR)\n",
    "        plot_image_grid([imgs['HR_np'], np.clip(out_HR_np, 0, 1)], factor=13, nrow=2)\n",
    "\n",
    "        print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psnr_LR) + \", \" + str(psnr_HR))\n",
    "\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "psnr_best_HR = 0\n",
    "best_image = 0\n",
    "best_iteration = 0\n",
    "psnr_history = [] \n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "\n",
    "i = 0\n",
    "p = get_params(OPT_OVER, net, net_input)\n",
    "optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
    "\n",
    "\n",
    "out_HR_np = np.clip(torch_to_np(best_image), 0, 1)\n",
    "\n",
    "plot_image_grid([imgs['HR_np'], out_HR_np], factor=13, nrow=2);\n",
    "\n",
    "print(str(best_iteration) + \" : \" + str(psnr_best_HR))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YG8u4QqFp81E",
    "colab_type": "text"
   },
   "source": [
    "### Hyperparameter Analysis\n",
    "\n",
    "The following code makes it possible to run super resolution with customized parameters and to plot graphs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vRKZ2kyopeSa",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def superResolution(img_path, label, LR = 0.01, factor = 4, num_iter = 2000, reg_noise_std = 0.03, NET_TYPE = 'skip'):\n",
    "\n",
    "    global i, net_input, psnr_best_HR, best_image, best_iteration\n",
    "\n",
    "\n",
    "    # Starts here\n",
    "    imgs = load_LR_HR_imgs_sr(img_path , imsize, factor, enforse_div32)\n",
    "    plot_image_grid([imgs['HR_np']], factor = 6)\n",
    "    plot_image_grid([imgs['LR_np']], factor = 6)\n",
    "\n",
    "\n",
    "    input_depth = 32\n",
    " \n",
    "    INPUT =     'noise'\n",
    "    pad   =     'reflection'\n",
    "    OPT_OVER =  'net'\n",
    "    KERNEL_TYPE='lanczos2'\n",
    "\n",
    "    tv_weight = 0.0\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    net_input = get_noise(input_depth, INPUT, (imgs['HR_pil'].size[1], imgs['HR_pil'].size[0])).type(dtype).detach()\n",
    "\n",
    "    net = get_net(input_depth, NET_TYPE, pad,\n",
    "                  skip_n33d=128, \n",
    "                  skip_n33u=128, \n",
    "                  skip_n11=4, \n",
    "                  num_scales=5,\n",
    "                  upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "    # Losses\n",
    "    mse = torch.nn.MSELoss().type(dtype)\n",
    "\n",
    "    img_LR_var = np_to_torch(imgs['LR_np']).type(dtype)\n",
    "\n",
    "    downsampler = Downsampler(n_planes=3, factor=factor, kernel_type=KERNEL_TYPE, phase=0.5, preserve_size=True).type(dtype)\n",
    "\n",
    "\n",
    "    def closure():\n",
    "        global i, net_input, psnr_best_HR, best_image, best_iteration\n",
    "    \n",
    "        if reg_noise_std > 0:\n",
    "            net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "\n",
    "        out_HR = net(net_input)\n",
    "        out_LR = downsampler(out_HR)\n",
    "\n",
    "        total_loss = mse(out_LR, img_LR_var) \n",
    "    \n",
    "        if tv_weight > 0:\n",
    "            total_loss += tv_weight * tv_loss(out_HR)\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "        # Log\n",
    "        psnr_LR = compare_psnr(imgs['LR_np'], torch_to_np(out_LR))\n",
    "        psnr_HR = compare_psnr(imgs['HR_np'], torch_to_np(out_HR))\n",
    "                      \n",
    "        # History\n",
    "        psnr_history.append(psnr_HR)\n",
    "    \n",
    "        if psnr_HR > psnr_best_HR:\n",
    "            psnr_best_HR = psnr_HR\n",
    "            best_image = out_HR\n",
    "            best_iteration = i\n",
    "\n",
    "        if PLOT and i % 100 == 0:\n",
    "            out_HR_np = torch_to_np(out_HR)\n",
    "            plot_image_grid([imgs['HR_np'], np.clip(out_HR_np, 0, 1)], factor=12, nrow=2)\n",
    "\n",
    "            print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psnr_LR) + \", \" + str(psnr_HR))\n",
    "\n",
    "            plt.plot(psnr_history, label = label)\n",
    "            plt.xticks(range(0, num_iter + 1, 500))\n",
    "            plt.ylabel('psnr')\n",
    "            plt.xlabel('iterations')\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    psnr_best_HR = 0\n",
    "    best_image = 0\n",
    "    best_iteration = 0\n",
    "    psnr_history = [] \n",
    "    net_input_saved = net_input.detach().clone()\n",
    "    noise = net_input.detach().clone()\n",
    "\n",
    "    i = 0\n",
    "    p = get_params(OPT_OVER, net, net_input)\n",
    "    optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
    "\n",
    "\n",
    "    out_HR_np = np.clip(torch_to_np(best_image), 0, 1)\n",
    "\n",
    "    plot_image_grid([imgs['HR_np'], out_HR_np], factor=12, nrow=2);\n",
    "    plot_image_grid([imgs['LR_np']], factor = 6)\n",
    "\n",
    "    print(str(best_iteration) + \" : \" + str(psnr_best_HR))\n",
    "\n",
    "    plt.plot(psnr_history, label = label)\n",
    "    plt.xticks(range(0, num_iter + 1, 500))\n",
    "    plt.ylabel('psnr')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return psnr_history, label\n",
    "    "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCkD7VydqoVy",
    "colab_type": "text"
   },
   "source": [
    "#### Learning rate\n",
    "We first test different learning rates with factor 4. \n",
    "We use the bird image as default image, because this image's resolution is not very high and therefore the time to generate a new image is not very long."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5X7ClwNSpfsj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "path_to_image = 'custom/Set5/bird.png'\n",
    "data1, label1 = superResolution(path_to_image, \"0.1\", LR= 0.1, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
    "data2, label2 = superResolution(path_to_image, \"0.01\", LR= 0.01, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
    "data3, label3 = superResolution(path_to_image, \"0.001\", LR= 0.001, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
    "data4, label4 = superResolution(path_to_image, \"0.0001\", LR= 0.0001, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
    "\n",
    "\n",
    "plt.plot(data1, label = label1)\n",
    "plt.plot(data2, label = label2)\n",
    "plt.plot(data3, label = label3)\n",
    "plt.plot(data4, label = label4)\n",
    "plt.xticks(range(0, 2001, 500))\n",
    "plt.ylabel('psnr')\n",
    "plt.xlabel('iterations')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suprisingly, we see that a learning rate of `0.001` has a better performance than the default of `0.01`.\n",
    "Next, we test different learning rates with factor 8.\n",
    "We use the same bird image for this task as a default image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wtSpXlN2c_yi",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "source": [
    "path_to_image = 'custom/Set5/bird.png'\n",
    "data1, label1 = superResolution(path_to_image, \"0.1\", LR= 0.1, factor = 8, reg_noise_std = 0.05, num_iter = 4000)\n",
    "data2, label2 = superResolution(path_to_image, \"0.01\", LR= 0.01, factor = 8, reg_noise_std = 0.05, num_iter = 4000)\n",
    "data3, label3 = superResolution(path_to_image, \"0.001\", LR= 0.001, factor = 8, reg_noise_std = 0.05, num_iter = 4000)\n",
    "data4, label4 = superResolution(path_to_image, \"0.0001\", LR= 0.0001, factor = 8, reg_noise_std = 0.05, num_iter = 4000)\n",
    "\n",
    "\n",
    "plt.plot(data1, label = label1)\n",
    "plt.plot(data2, label = label2)\n",
    "plt.plot(data3, label = label3)\n",
    "plt.plot(data4, label = label4)\n",
    "plt.xticks(range(0, 4001, 500))\n",
    "plt.ylabel('psnr')\n",
    "plt.xlabel('iterations')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to the results of a factor 4, a learning rate of `0.001` has a better performance than the default `0.01`. \n",
    "The reason for this is that for generating an image with a higher learning rate does not converge to an optimal solution.\n",
    "Smaller learning steps are needed to create a better representation of the original image.\n",
    "We can also see that for a factor 8, the image does not improve significantly after 2000 iterations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Different factors\n",
    "Here we try different scaling factors for the bird image. \n",
    "We try to see the difference in performance for different factors of scaling to see what the limit is for generating good images. \n",
    "For each factor, the learning rate is set to 0.001 and the number of iterations to 2000."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_image = 'custom/Set5/bird.png'\n",
    "data1, label1 = superResolution(path_to_image, \"2\", LR= 0.001, factor = 2, reg_noise_std = 0.05, num_iter = 2000)\n",
    "data2, label2 = superResolution(path_to_image, \"4\", LR= 0.001, factor = 4, reg_noise_std = 0.05, num_iter = 2000)\n",
    "data3, label3 = superResolution(path_to_image, \"8\", LR= 0.001, factor = 8, reg_noise_std = 0.05, num_iter = 2000)\n",
    "data4, label4 = superResolution(path_to_image, \"16\", LR= 0.001, factor = 16, reg_noise_std = 0.05, num_iter = 2000)\n",
    "data5, label5 = superResolution(path_to_image, \"32\", LR= 0.001, factor = 32, reg_noise_std = 0.05, num_iter = 2000)\n",
    "\n",
    "\n",
    "plt.plot(data1, label = label1)\n",
    "plt.plot(data2, label = label2)\n",
    "plt.plot(data3, label = label3)\n",
    "plt.plot(data4, label = label4)\n",
    "plt.plot(data5, label = label5)\n",
    "plt.xticks(range(0, 2001, 500))\n",
    "plt.ylabel('psnr')\n",
    "plt.xlabel('iterations')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that smaller images, so images with a higher factor, converge faster to their optimal solution and that the PSNR is much lower. \n",
    "This was also expected.\n",
    "From down sampling with a factor higher than 8 the image gets too blurry and it is hard to regenerate the original image."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Different networks\n",
    "Here we compare the UNet network with the skip network. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_image = 'custom/Set5/bird.png'\n",
    "data1, label1 = superResolution(path_to_image, \"skip\", LR= 0.001, factor = 8, reg_noise_std = 0.05, num_iter = 2000, NET_TYPE = 'skip')\n",
    "data2, label2 = superResolution(path_to_image, \"UNet\", LR= 0.001, factor = 8, reg_noise_std = 0.05, num_iter = 2000, NET_TYPE = 'UNet')\n",
    "\n",
    "\n",
    "plt.plot(data1, label = label1)\n",
    "plt.plot(data2, label = label2)\n",
    "plt.xticks(range(0, 2001, 500))\n",
    "plt.ylabel('psnr')\n",
    "plt.xlabel('iterations')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is clear that the default skip network performs better than the UNet network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deypCvEkLGvq",
    "colab_type": "text"
   },
   "source": [
    "## Conclusions\n",
    "From the experiments run above we can conclude that indeed the results from the original Deep Image Prior paper can be replicated with their code and that the results are very dependend on the choice of hyperparameters. Hyperparameter tweaking is needed when trying this method on a different set of images.\n",
    "\n",
    "Another important conclusion is that indeed these convolutional networks have a strong tendency to prefer natural structures. This behavior seems even stronger than indicated. The sudden drops in the PSNR values are specific and it not sure why this happens.\n",
    "\n",
    "The claim in table 1 of the original article that the Deep Image Prior performs better than the indicated alternative solution are not reproducable. Especially because the variations in the PSNR are considerable and larger than the published improvement in table 1.\n",
    "\n",
    "Finally, we see that that the hyper parameters for super resolution set by Ulyanov et al. perform near optimal.\n",
    "The only improvement that we found is to set the learning rate to `0.001` instead of `0.01`.\n",
    "For the best network, the skip network should be used over UNet and for small images like the bird image it is hard to reproduce a natural image when the factor gets higher than 8.\n"
   ]
  }
 ]
}