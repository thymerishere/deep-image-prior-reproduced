{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-image-prior-reproduced-github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thymerishere/deep-image-prior-reproduced/blob/master/deep_image_prior_reproduced_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdxzgRMOGWBo",
        "colab_type": "text"
      },
      "source": [
        "# Analysis of Deep Image Prior\n",
        "In this notebook we analyse different characteristics of several applications as described in the [original Deep Image Prior paper](https://arxiv.org/abs/1711.10925) published by Ulyanov et al.\n",
        "\n",
        "We looked at four different aspects:\n",
        "1. reproduce inpainting and analyse consequences of different settings of hyperparameters\n",
        "2. run the restoration/inpainting application code for new images with different settings of the hyperparameters to check which gives the best results\n",
        "3. test how the setup of skip connections in the network influence the performance of restoration logic on selected images\n",
        "4. reproduce table 1/figure 7 from the [original paper](https://arxiv.org/abs/1711.10925), also looking at how the reported value (PSNR) evolves over time\n",
        "\n",
        "Each of these tasks is contained in its own section below with separate description and code.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNni_08IHbfN",
        "colab_type": "text"
      },
      "source": [
        "## How does Deep Image Prior work?\n",
        "With deep convolutional neural nets (ConvNets) being the state of the art in reverse image reconstruction, Ulyanov et al. [argue](https://arxiv.org/abs/1711.10925) that the assumption that the excellent performance is obtained due to the ability of ConvNets to learn realistic image priors from data, is flawed. They aim do demonstrate that image reconstruction can be done using only the information of the single degraded input image.\n",
        "This is done by interpreting the generator neural network as a parameterisation $x=f_\\theta(z)$ of an image $x \\in \\mathbb{R}^{3\\times H \\times W}$ with $z\\in \\mathbb{R}^{C' \\times H' \\times W'}$ a randomly initialised code vector. This then gives the following optimisation: $$\\theta^* = argmin_\\theta E(f_\\theta(z); x_0), \\quad x^*=f_{\\theta^*}(z)$$\n",
        "where, using gradient descent and starting from a random initialisation of the parameters, the aim is to achieve the optimal parameters $\\theta^*$ to obtain the optimal mapping of noise to output image $x^*=f_{\\theta^*}(z)$. The data term $E(x, x_0)$ is dependent on the task performed where Ulyanov et al discussed denoising, generic reconstruction (JPEG restoration), super-resolution, inpainting, restoration, natural pre-image and flash-no flash reconstruction. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G4qS11iIEoK",
        "colab_type": "text"
      },
      "source": [
        "## Experimental Setup\n",
        "The authors performed all tasks on images from the [GCF-BM3D dataset](http://www.cs.tut.fi/~foi/GCF-BM3D/) using varying hyperparameters which are explained in more depth in the [original paper's supplementary](https://dmitryulyanov.github.io/deep_image_prior). The code Ulyanov et al. have written to obtain their results in the original paper is [fully provided](https://github.com/DmitryUlyanov/deep-image-prior) and we use their code to obtain similar results on three of the tasks of the paper. We will also use a custom made set of images to view the performance on a different dataset. Next we will discuss how to setup the experiment.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FREet7bqHV2N",
        "colab_type": "text"
      },
      "source": [
        "## Running this Experiment on Google Colab\n",
        "In order to run this experiment, a few steps need to be taken in order to load the files needed. First, get the [original Deep Image Prior project from github](https://dmitryulyanov.github.io/deep_image_prior) and put it inside the root folder of your Google Drive home folder. Then, download the provided extra dataset from our github repository and put it inside `deep-image-prior-master/data`. This following code snippet will then mount your Google Drive and navigate to the project folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy_5raUwFirA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/My\\ Drive/deep-image-prior-master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxAG9-Ooc2Cl",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HftsGcfYJik3",
        "colab_type": "text"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "We analysed three tasks of the Deep Image Prior: the inpainting task, the restoration task and the Super Resolution task.\n",
        "\n",
        "In the coming three sections we will reiterate what the function is of each task, how it achieves its functionality, whether the results from the original paper can be replicated using the original code and additional analysis of the parameters within each task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRUwANrAKYOo",
        "colab_type": "text"
      },
      "source": [
        "### Inpainting\n",
        "Image inpainting is specified by Ulyanov et al. as reconstructing missing data given an image $x_0$ with missing pixels denoted by a binary mask $m \\in \\{0,1\\}^{H \\times W}$. This gives a data term of $$E(x;x_0)=||(x-x_0) \\odot m||^2$$ with $\\odot$ being Hadamard's product. This data term is then used in the optimisation objective discussed previously. The task is specified in the code supplied by Ulyanov et al. with a number of default hyper-parameters. We will discuss the reproduction process wherein we reproduce Figure 7 and 8 from the original paper and because no reasoning has been given on the chosen values for the hyper-parameters we will try to discuss some of them and their influence on the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyCDMF1Sw0BT",
        "colab_type": "text"
      },
      "source": [
        "#### Replication\n",
        "Using the following [code](https://github.com/DmitryUlyanov/deep-image-prior) from the original paper we can perform the inpainting tasks. Uncommenting either the location to the vase image and its mask or the Kate image and its mask replicates respectively the results from Figure 6 and 7 in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN-atG0GH0VD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from models.resnet import ResNet\n",
        "from models.unet import UNet\n",
        "from models.skip import skip\n",
        "import torch\n",
        "import torch.optim\n",
        "\n",
        "from utils.inpainting_utils import *\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "dtype = torch.cuda.FloatTensor\n",
        "\n",
        "PLOT = True\n",
        "imsize = -1\n",
        "dim_div_by = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Ty-sttH9Of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### Select the correct image.\n",
        "\n",
        "# Uncomment for vase.\n",
        "img_path  = 'data/inpainting/vase.png'\n",
        "mask_path = 'data/inpainting/vase_mask.png'\n",
        "\n",
        "# Uncomment for Kate.\n",
        "# img_path  = 'data/inpainting/kate.png'\n",
        "# mask_path = 'data/inpainting/kate_mask.png'\n",
        "\n",
        "##############\n",
        "\n",
        "# Creating the image and image mask.\n",
        "img_pil, img_np = get_image(img_path, imsize)\n",
        "img_mask_pil, img_mask_np = get_image(mask_path, imsize)\n",
        "\n",
        "img_mask_pil = crop_image(img_mask_pil, dim_div_by)\n",
        "img_pil      = crop_image(img_pil,      dim_div_by)\n",
        "\n",
        "img_np      = pil_to_np(img_pil)\n",
        "img_mask_np = pil_to_np(img_mask_pil)\n",
        "\n",
        "# Visualising the task.\n",
        "img_mask_var = np_to_torch(img_mask_np).type(dtype)\n",
        "plot_image_grid([img_np, img_mask_np, img_mask_np*img_np], 3,11);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ftQ5g5IB_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting hyperparameters. Automatically selects the hyperparameters\n",
        "# for the corresponding task.\n",
        "pad = 'reflection' # 'zero'\n",
        "OPT_OVER = 'net'\n",
        "OPTIMIZER = 'adam'\n",
        "\n",
        "if 'vase.png' in img_path:\n",
        "    INPUT = 'meshgrid'\n",
        "    input_depth = 2\n",
        "    LR = 0.01 \n",
        "    num_iter = 5001\n",
        "    param_noise = False\n",
        "    show_every = 50\n",
        "    figsize = 5\n",
        "    reg_noise_std = 0.03\n",
        "    \n",
        "    net = skip(input_depth, img_np.shape[0], \n",
        "               num_channels_down = [128] * 5,\n",
        "               num_channels_up   = [128] * 5,\n",
        "               num_channels_skip = [0] * 5,  \n",
        "               upsample_mode='nearest', filter_skip_size=1, filter_size_up=3, filter_size_down=3,\n",
        "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
        "    \n",
        "elif ('kate.png' in img_path):\n",
        "    # Hyperparameters are the same as super resolution.\n",
        "    INPUT = 'noise'\n",
        "    input_depth = 32\n",
        "    LR = 0.01 \n",
        "    num_iter = 6001\n",
        "    param_noise = False\n",
        "    show_every = 50\n",
        "    figsize = 5\n",
        "    reg_noise_std = 0.03\n",
        "    \n",
        "    net = skip(input_depth, img_np.shape[0], \n",
        "               num_channels_down = [128] * 5,\n",
        "               num_channels_up =   [128] * 5,\n",
        "               num_channels_skip =    [128] * 5,  \n",
        "               filter_size_up = 3, filter_size_down = 3, \n",
        "               upsample_mode='nearest', filter_skip_size=1,\n",
        "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
        "\n",
        "net = net.type(dtype)\n",
        "net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype)\n",
        "\n",
        "# Compute number of parameters\n",
        "s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
        "print ('Number of params: %d' % s)\n",
        "\n",
        "# Setting loss\n",
        "mse = torch.nn.MSELoss().type(dtype)\n",
        "\n",
        "img_var = np_to_torch(img_np).type(dtype)\n",
        "mask_var = np_to_torch(img_mask_np).type(dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkCcL1A7wwvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main loop.\n",
        "i = 0\n",
        "def closure():\n",
        "    \n",
        "    global i\n",
        "\n",
        "    if param_noise:\n",
        "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
        "            n = n + n.detach().clone().normal_() * n.std() / 50\n",
        "    \n",
        "    net_input = net_input_saved\n",
        "    if reg_noise_std > 0:\n",
        "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "        \n",
        "        \n",
        "    out = net(net_input)\n",
        "   \n",
        "    total_loss = mse(out * mask_var, img_var * mask_var)\n",
        "    total_loss.backward()\n",
        "        \n",
        "    print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
        "    if  PLOT and i % show_every == 0:\n",
        "        print(\"Iteration \", i)\n",
        "        out_np = torch_to_np(out)\n",
        "        plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
        "        \n",
        "    i += 1\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "net_input_saved = net_input.detach().clone()\n",
        "noise = net_input.detach().clone()\n",
        "\n",
        "p = get_params(OPT_OVER, net, net_input)\n",
        "optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
        "i = 0\n",
        "\n",
        "# Printing the result to the screen.\n",
        "\n",
        "out_np = torch_to_np(net(net_input))\n",
        "plot_image_grid([out_np], factor=5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI9e7egryw4o",
        "colab_type": "text"
      },
      "source": [
        "We can see that indeed we obtain a very similar results to the those presented in the original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2hiV6aY_5-t",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter Analysis\n",
        "The [original paper](https://arxiv.org/abs/1711.10925) specifies, besides the network structure which will be discussed in the section on restoration, six hyperparameters:\n",
        "\n",
        "| Hyperparameter | Name in the code | Default value |\n",
        "| -------------- | ---------------- | ------------- |\n",
        "| Amount of input channels | `input_depth` | 2 for vase, 32 for Kate |\n",
        "| Network input type | `INPUT` | Mesh for vase, noise for Kate|\n",
        "| Standard deviation of the input noise $\\sigma_p$ | `reg_noise_std`| $\\frac{1}{20}$ |\n",
        "| Number of iterations | `num_iter` | 5000 |\n",
        "| Learning rate | `LR` | 0.1 |\n",
        "\n",
        "As the [original paper](https://arxiv.org/abs/1711.10925) states that its main aim is not to maximise PSNR, we will analyse the effect of the hyperparameters on the visual quality of the result on images from a dataset we created for this analysis. The following method allows us to run their code with different hyperparameters, images and masks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qutekcONS4NB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defines inpainting method with default values.\n",
        "def inpainting(img_path, mask_path, INPUT='noise', input_depth=2, LR=0.01,\n",
        "               num_iter=5000, reg_noise_std=0.05):\n",
        "\n",
        "    # Creating the image and image mask.\n",
        "    img_pil, img_np = get_image(img_path, imsize)\n",
        "    img_mask_pil, img_mask_np = get_image(mask_path, imsize)\n",
        "\n",
        "    img_mask_pil = crop_image(img_mask_pil, dim_div_by)\n",
        "    img_pil      = crop_image(img_pil,      dim_div_by)\n",
        "\n",
        "    img_np      = pil_to_np(img_pil)\n",
        "    img_mask_np = pil_to_np(img_mask_pil)\n",
        "\n",
        "    # Visualising the task.\n",
        "    img_mask_var = np_to_torch(img_mask_np).type(dtype)\n",
        "    plot_image_grid([img_np, img_mask_np, img_mask_np*img_np], 3,11);\n",
        "\n",
        "    param_noise = False\n",
        "    show_every = 250\n",
        "    figsize = 5\n",
        "\n",
        "    pad = 'reflection' # 'zero'\n",
        "    OPT_OVER = 'net'\n",
        "    OPTIMIZER = 'adam'\n",
        "\n",
        "    net = skip(input_depth, img_np.shape[0], \n",
        "                num_channels_down = [128] * 5,\n",
        "                num_channels_up =   [128] * 5,\n",
        "                num_channels_skip =    [128] * 5,  \n",
        "                filter_size_up = 3, filter_size_down = 3, \n",
        "                upsample_mode='nearest', filter_skip_size=1,\n",
        "                need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
        "\n",
        "    net = net.type(dtype)\n",
        "    net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype)\n",
        "\n",
        "    # Compute number of parameters\n",
        "    s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
        "    print ('Number of params: %d' % s)\n",
        "\n",
        "    # Setting loss\n",
        "    mse = torch.nn.MSELoss().type(dtype)\n",
        "\n",
        "    img_var = np_to_torch(img_np).type(dtype)\n",
        "    mask_var = np_to_torch(img_mask_np).type(dtype)\n",
        "\n",
        "    # Main loop.\n",
        "    global i\n",
        "    i = 0\n",
        "    def closure():\n",
        "        \n",
        "        global i\n",
        "\n",
        "        if param_noise:\n",
        "            for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
        "                n = n + n.detach().clone().normal_() * n.std() / 50\n",
        "        \n",
        "        net_input = net_input_saved\n",
        "        if reg_noise_std > 0:\n",
        "            net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "            \n",
        "            \n",
        "        out = net(net_input)\n",
        "    \n",
        "        total_loss = mse(out * mask_var, img_var * mask_var)\n",
        "        total_loss.backward()\n",
        "            \n",
        "        print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
        "        if  PLOT and i % show_every == 0:\n",
        "            print(\"Iteration \", i)\n",
        "            out_np = torch_to_np(out)\n",
        "            plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
        "            \n",
        "        i += 1\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    net_input_saved = net_input.detach().clone()\n",
        "    noise = net_input.detach().clone()\n",
        "\n",
        "    p = get_params(OPT_OVER, net, net_input)\n",
        "    optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
        "\n",
        "    # Printing the result to the screen.\n",
        "\n",
        "    out_np = torch_to_np(net(net_input))\n",
        "    plot_image_grid([out_np], factor=5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCA7jdZA3C9c",
        "colab_type": "text"
      },
      "source": [
        "##### Number of iterations\n",
        "The number of iterations determines the amount of time steps the optimiser gets to optimise the over the objective function. The default amount of iterations is set to 5000. The [original paper](https://arxiv.org/abs/1711.10925) discusses the amount of iterations with regard to restoration and that eventualy the network will overfit to the image and thus reintroduce the artifacts of which it is the goal to remove. For inpainting it is not discussed what the repercussions of high number of iterations are. We expect that the image quality will improve up to a point where more learning adds no additional quality as it has learned on all known pixels. For this task we set the iterations to 15000 and determine when the image quality stops improving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv5uyBnY3Gsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", num_iter=15000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wvc0RpE3LWU",
        "colab_type": "text"
      },
      "source": [
        "It is clear that, as expected, after the first ~3000 iterations there is no significant improvement in image quality. It seems that the network resets a couple of times within the optimisation process which is explained by the [original paper's supplementary](https://dmitryulyanov.github.io/deep_image_prior): \"We found the optimization process tends to destabilize as the loss goes down and approaches a certain value. \\[...] To remedy this issue we simply track the optimization loss and return to parameters from the previous iteration if the loss difference between two consecutive iterations is higher than a certain threshold.\" After the resets the learning seems to bring the quality back to the point before the reset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFr7QJdE3ThS",
        "colab_type": "text"
      },
      "source": [
        "##### Learning rate\n",
        "The learning rate influences how much the network learns per iteration. The [original paper](https://arxiv.org/abs/1711.10925) states that the result is sensitive to the choice of learning rate as in the vase inpainting, with learning rate equal to $10^{-4}$, the location the mask was clearly visible. Therefore, for most tasks, a learning rate of $10^{-3}$ was chosen. In our example we expect to find a similar result where with too low learning rates the network will not be able to learn the known pixels in a reasonable time, while with too high learning rates the network will not be able to generate the known pixels accurately and always generate blurry images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI6bROxw3Wzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", LR=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5lSanSZ3Yh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", LR=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFtxU6KE3aIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", LR=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5kxkDc23dN4",
        "colab_type": "text"
      },
      "source": [
        "Interstingly, with high learning rates, the network converges to a black image, while with very low learning rates the image gets increasingly blurrier, possible due to needing longer to converge. A low learning rate clearly blends the missing pixels better with the known pixels though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWSZmqLuTa2B",
        "colab_type": "text"
      },
      "source": [
        "##### Input depth\n",
        "The `input_depth` parameter changes the amount of input channels to the image. We would then expect the network to learn more features which would in turn lead to a better blend of the filled pixels as the network learned more of the masked image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YpJUHhqT3uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", input_depth=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkluxwso3muv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", input_depth=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-nA-7Uv--2",
        "colab_type": "text"
      },
      "source": [
        "From both runs we can conclude that, as expected, with a higher input depth the filled in pixels are less noisy and blend in better with the rest of the image. An interesting observation is that the number plate of the car is arguably more realistic when the amount of input channels is higher, as it is sharper, even though the filled in pixels do not represent any letters. Furthermore, the edges on the left headlamp are more defined with higher input depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6OHfyM6yM7G",
        "colab_type": "text"
      },
      "source": [
        "#### Network input type\n",
        "The network input type defines what the network receives as input from which it tries to reconstruct the masked image. The [original paper](https://arxiv.org/abs/1711.10925) presents two options: a mesh grid, which is used for the 'vase' example, and noise, which is used for the 'Kate' example. The mesh grid consists two channels of black to white gradients with one going left to right and one going top to bottom. The [original paper' supplementary material](https://dmitryulyanov.github.io/deep_image_prior) states that the mesh grid was deemed only useful when doing large-hole inpainting and not for other tasks. We expect this the case due to noise possibly being better at masking the edges of the mask. The following code snippet runs the inpainting task on the 'car' image with both input types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54zc_HDy3jsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", INPUT='meshgrid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOp7BKJc8alq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", INPUT='noise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPYwMw9V-neE",
        "colab_type": "text"
      },
      "source": [
        "It is clear from the results that with the traces of the mask are more defined when the mesh is used instead of noise, as expected. Also, when using noise the image is sharper and more defined than when using the mesh grid as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxTT1ujk3y-o",
        "colab_type": "text"
      },
      "source": [
        "##### Input noise standard deviation\n",
        "When `reg_noise_std` is greater than 0, at each iteration additional normal noise with a factor of `reg_noise_std` is added to the input of the network. The [original paper's supplementary](https://dmitryulyanov.github.io/deep_image_prior) notes that this is used as a regularisation which impedes the optimisation process, but that the network will always optimise the objective to 0. We are more interested in the visual effect of this regularisation technique and the following code snippets perform the inpainting with different values for `reg_noise_std`. We expect the additional noise to make the network more robust to noise and therefore to create a smoother image which better blends the borders between the known and unknown pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDpllKW_4Fid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", reg_noise_std=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwwGl1Tf4GJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", reg_noise_std=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdhMaSmP4JLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/tesla.png\", \"data/custom/mask3.png\", reg_noise_std=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHlz7NPP4NEY",
        "colab_type": "text"
      },
      "source": [
        "It is clear that when no added input noise is used, the mask is clearly visible which returns a visually very unappealing image, but when the added noise is too much the optimiser has difficulties even optimising over the known pixels. A value of 0.1 indicates that it is best to have something in between, which is to be expected as [too much regularasation makes the data lose important properties](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK5TBYewvd5V",
        "colab_type": "text"
      },
      "source": [
        "#### Other images\n",
        "The following code snippets will run the network on other images from the custom dataset. The first image is of a large group of people with a larger mask which requires a large amount of detailed inpainting. We will try two different approaches. First we will use noise with a low learning rate, high added input noise and high input depth to try and create a smooth image. For the second approach we will try and replicate the busy environment with a high learning rate and less smoothing and lower input depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AFJybECvyA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/people2.png\", \"data/custom/mask2.png\", reg_noise_std=0.2, LR=0.001, input_depth=32, INPUT='noise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trbg2stSWuYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/people2.png\", \"data/custom/mask2.png\", reg_noise_std=0.05, LR=0.1, input_depth=2, INPUT='noise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL5iGW72WgI5",
        "colab_type": "text"
      },
      "source": [
        "The results of both runs really vary only in the way they painted in the missing pixels. The first run has more blurry pixels while the second run is has more noisy pixels. This blurriness let us decide on the second result being more realistic.\n",
        "\n",
        "The next picture consists of a grass field with a sky with clouds. We will again use the same to approaches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ydhKKCl4zwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/grass.png\", \"data/custom/mask2.png\", reg_noise_std=0.2, LR=0.001, input_depth=32, INPUT='noise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEjPTHHA40L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inpainting(\"data/custom/grass.png\", \"data/custom/mask2.png\", reg_noise_std=0.05, LR=0.1, input_depth=2, INPUT='noise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qob65ng4R2gw",
        "colab_type": "text"
      },
      "source": [
        "It is clear that the first set of hyperparameters achieve a better result. It is very hard to determine where the mask has been. The second set of hyperparameters makes the learning very unstable which leads to a very blurry image, or no result at all where only a black image is generated. This is probably due to the learning rate being too high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSYj5Si_KaN5",
        "colab_type": "text"
      },
      "source": [
        "## Determining the influence of different Skip connection settings\n",
        "\n",
        "The restoration NN used in the Deep Image Prior paper has a U-Net form with skip connections. The skip connections are assumed to influence the performance in both result as well as speed of learning. This code in this section will try to analyse this influence by testing different network layouts with less or no skip connections. The test is done on two images (barbara.png and man.png) to make it more feasible to compare with the results of the standard restoration by Deep Image Prior.\n",
        "\n",
        "The code below is mostly copied from this [2] ipynb, changes have been made to allow running with different skip connection setups.\n",
        "\n",
        "To run the code uncomment the line with the image you want to try, and run the cells in this section one by one, with the last cell being the call to the testSkipConnectionChanges method.\n",
        "\n",
        "[1]\n",
        "https://github.com/DmitryUlyanov/deep-image-prior/blob/master/restoration.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT4W_gNmYroh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment the image you want to test. Note this code assumes that the images are located in the \n",
        "# data/restoration folder inside your google drive.\n",
        "#f = './data/restoration/barbara.png'\n",
        "f = './data/restoration/man.png'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhF9XsEhYtdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from utils.inpainting_utils import *\n",
        "\n",
        "# Init\n",
        "psrns = []\n",
        "psrns_masked = []\n",
        "max_psrn = 0\n",
        "max_iter = 0\n",
        "\n",
        "last_net = None\n",
        "psrn_masked_last = 0\n",
        "i = 0\n",
        "\n",
        "def testSkipConnectionChanges():\n",
        "\n",
        "  #os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "\n",
        "  from models.resnet import ResNet\n",
        "  from models.unet import UNet\n",
        "  from models.skip import skip\n",
        "  from models import get_net\n",
        "  from skimage.measure import compare_psnr\n",
        "\n",
        "  torch.backends.cudnn.enabled = True\n",
        "  torch.backends.cudnn.benchmark =True\n",
        "  dtype = torch.cuda.FloatTensor\n",
        "\n",
        "  PLOT = True\n",
        "  imsize=-1\n",
        "  dim_div_by = 64\n",
        "  dtype = torch.cuda.FloatTensor\n",
        "\n",
        "  img_pil, img_np = get_image(f, imsize)\n",
        "\n",
        "  img_np = nn.ReflectionPad2d(1)(np_to_torch(img_np))[0].numpy()\n",
        "  img_pil = np_to_pil(img_np)\n",
        "  \n",
        "  img_mask = get_bernoulli_mask(img_pil, 0.50)\n",
        "  img_mask_np = pil_to_np(img_mask)\n",
        "\n",
        "  img_masked = img_np * img_mask_np\n",
        "\n",
        "  mask_var = np_to_torch(img_mask_np).type(dtype)\n",
        "\n",
        "  plot_image_grid([img_np, img_mask_np, img_mask_np * img_np], 3,11);\n",
        "\n",
        "  show_every=500\n",
        "  figsize=5\n",
        "  pad = 'reflection' # 'zero'\n",
        "  INPUT = 'noise'\n",
        "  input_depth = 32\n",
        "  OPTIMIZER = 'adam'\n",
        "  OPT_OVER =  'net'\n",
        "  \n",
        "  LR = 0.001\n",
        "  num_iter = 11001\n",
        "  reg_noise_std = 0.03\n",
        "  \n",
        "  NET_TYPE = 'skip'\n",
        "  net = get_net(input_depth, 'skip', pad, n_channels=1,\n",
        "                skip_n33d=128, \n",
        "                skip_n33u=128, \n",
        "                skip_n11=4, \n",
        "                num_scales=5,\n",
        "                upsample_mode='bilinear').type(dtype)\n",
        "      \n",
        "  # Loss\n",
        "  mse = torch.nn.MSELoss().type(dtype)\n",
        "  img_var = np_to_torch(img_np).type(dtype)\n",
        "\n",
        "  net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype).detach()\n",
        "\n",
        "  def closure():\n",
        "\n",
        "    global i, psrn_masked_last, last_net, net_input, max_psrn, max_iter, psrns, psrns_masked\n",
        "    \n",
        "    if reg_noise_std > 0:\n",
        "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "    \n",
        "    out = net(net_input)\n",
        "\n",
        "    total_loss = mse(out * mask_var, img_var * mask_var)\n",
        "    total_loss.backward()\n",
        "    \n",
        "    psrn_masked = compare_psnr(img_masked, out.detach().cpu().numpy()[0] * img_mask_np) \n",
        "    psrn = compare_psnr(img_np, out.detach().cpu().numpy()[0]) \n",
        "\n",
        "    psrns.append(psrn)\n",
        "    psrns_masked.append(psrn_masked)\n",
        "    if psrn > max_psrn:\n",
        "      max_psrn = psrn\n",
        "      max_iter = i\n",
        "    print ('Iteration %05d    Loss %f PSNR_masked %f PSNR %f' % (i, total_loss.item(), psrn_masked, psrn),'\\r', end='')\n",
        "    \n",
        "    \n",
        "    if  PLOT and i % show_every == 0:\n",
        "        out_np = torch_to_np(out)\n",
        "        \n",
        "        # Backtracking\n",
        "        if psrn_masked - psrn_masked_last < -5: \n",
        "            print('Falling back to previous checkpoint.')\n",
        "\n",
        "            for new_param, net_param in zip(last_net, net.parameters()):\n",
        "                net_param.data.copy_(new_param.cuda())\n",
        "\n",
        "            return total_loss*0\n",
        "        else:\n",
        "            last_net = [x.cpu() for x in net.parameters()]\n",
        "            psrn_masked_last = psrn_masked\n",
        "\n",
        "\n",
        "        print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psrn))\n",
        "\n",
        "        plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "  net_input_saved = net_input.detach().clone()\n",
        "  noise = net_input.detach().clone()\n",
        "\n",
        "  # Run\n",
        "  p = get_params(OPT_OVER, net, net_input)\n",
        "  optimize(OPTIMIZER, p, closure, LR=LR, num_iter=num_iter)\n",
        "\n",
        "  print(max_psrn)\n",
        "  print(max_iter)\n",
        "  out_np = torch_to_np(net(net_input))\n",
        "  q = plot_image_grid([np.clip(out_np, 0, 1), img_np], factor=13);\n",
        "\n",
        "  plt.plot(psrns)\n",
        "  plt.xticks(range(0, 11001, 500))\n",
        "  plt.ylabel('psnr')\n",
        "  plt.xlabel('iterations')\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojHB5VDDYz74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testSkipConnectionChanges()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qchzft9D4Mgm",
        "colab_type": "text"
      },
      "source": [
        "## Reproducing Table 1 - Figure 7 (bottom)\n",
        "\n",
        "The [Deep Image Prior paper](https://arxiv.org/abs/1711.10925) compares its method of restoring images corrupted with a 50% noise mask with an approach using convolutional sparse coding [1]. The results are presented in table 1 of the Deep Image Prior article.  The claim made by the authors is that the deep image prior performs better than the approach of [1]. In this section we will try to reproduce the deep-image-prior results. \n",
        "\n",
        "Table 1 in the [Deep Image Prior paper](https://arxiv.org/abs/1711.10925) displays PSNR (Peak Signal to Noise Ratio) values. The question which comes up is how these PSNR values are determined. This because the Deep Image Prior approach means that during training of the neural network different PSNR values are obtained and PSNR values can vary up and down during the iterations. \n",
        "\n",
        "In addition the code in the paper includes a fall back step. If the network deviates too strongly in the wrong direction it will fall back to a previous point and restart the training from there. \n",
        "\n",
        "To get a feel for both the development of the PSNR and the fallback consequences we keep track and then plot the PSNR values for each iteration.\n",
        "\n",
        "The code below is mostly copied from this [2] ipynb, with extra code added to load the images, place all in one cell in this notebook and plot the PSNR values.\n",
        "\n",
        "To run the code, uncomment the line with the image you want to try, and run the cells in this section one by one, with the last cell being the call to the reproduce method.\n",
        "\n",
        "[1] V. Papyan, Y. Romano, J. Sulam, and M. Elad. Convolutional dictionary learning via local processing. In Proc.\n",
        "ICCV, 2017.\n",
        "\n",
        "[2]\n",
        "https://github.com/DmitryUlyanov/deep-image-prior/blob/master/restoration.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g13XbambXe5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment the image you want to test. Note this code assumes that the images are located in the \n",
        "# data/restoration folder inside your google drive.\n",
        "#f = './data/restoration/barbara.png'\n",
        "#f = './data/restoration/man.png'\n",
        "#f = './data/restoration/montage.png'\n",
        "#f = './data/restoration/hill.png'\n",
        "#f = './data/restoration/boat.png'\n",
        "#f = './data/restoration/house.png'\n",
        "#f = './data/restoration/Lena512.png'\n",
        "#f = './data/restoration/Cameraman256.png'\n",
        "#f = './data/restoration/peppers256.png'\n",
        "#f = './data/restoration/fingerprint.png'\n",
        "f = './data/restoration/couple.png'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJzWauqI7LKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reproducing table 1\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from utils.inpainting_utils import *\n",
        "\n",
        "# Init\n",
        "psrns = []\n",
        "psrns_masked = []\n",
        "max_psrn = 0\n",
        "max_iter = 0\n",
        "\n",
        "last_net = None\n",
        "psrn_masked_last = 0\n",
        "i = 0\n",
        "\n",
        "def reproduce():\n",
        "\n",
        "  #os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "\n",
        "  from models.resnet import ResNet\n",
        "  from models.unet import UNet\n",
        "  from models.skip import skip\n",
        "  from models import get_net\n",
        "  from skimage.measure import compare_psnr\n",
        "\n",
        "  torch.backends.cudnn.enabled = True\n",
        "  torch.backends.cudnn.benchmark =True\n",
        "  dtype = torch.cuda.FloatTensor\n",
        "\n",
        "  PLOT = True\n",
        "  imsize=-1\n",
        "  dim_div_by = 64\n",
        "  dtype = torch.cuda.FloatTensor\n",
        "\n",
        "  img_pil, img_np = get_image(f, imsize)\n",
        "\n",
        "  img_np = nn.ReflectionPad2d(1)(np_to_torch(img_np))[0].numpy()\n",
        "  img_pil = np_to_pil(img_np)\n",
        "  \n",
        "  img_mask = get_bernoulli_mask(img_pil, 0.50)\n",
        "  img_mask_np = pil_to_np(img_mask)\n",
        "\n",
        "  img_masked = img_np * img_mask_np\n",
        "\n",
        "  mask_var = np_to_torch(img_mask_np).type(dtype)\n",
        "\n",
        "  plot_image_grid([img_np, img_mask_np, img_mask_np * img_np], 3,11);\n",
        "\n",
        "  show_every=500\n",
        "  figsize=5\n",
        "  pad = 'reflection' # 'zero'\n",
        "  INPUT = 'noise'\n",
        "  input_depth = 32\n",
        "  OPTIMIZER = 'adam'\n",
        "  OPT_OVER =  'net'\n",
        "  \n",
        "  LR = 0.001\n",
        "  num_iter = 11001\n",
        "  reg_noise_std = 0.03\n",
        "  \n",
        "  NET_TYPE = 'skip'\n",
        "  net = get_net(input_depth, 'skip', pad, n_channels=1,\n",
        "                skip_n33d=128, \n",
        "                skip_n33u=128, \n",
        "                skip_n11=4, \n",
        "                num_scales=5,\n",
        "                upsample_mode='bilinear').type(dtype)\n",
        "      \n",
        "  # Loss\n",
        "  mse = torch.nn.MSELoss().type(dtype)\n",
        "  img_var = np_to_torch(img_np).type(dtype)\n",
        "\n",
        "  net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype).detach()\n",
        "\n",
        "  def closure():\n",
        "\n",
        "    global i, psrn_masked_last, last_net, net_input, max_psrn, max_iter, psrns, psrns_masked\n",
        "    \n",
        "    if reg_noise_std > 0:\n",
        "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "    \n",
        "    out = net(net_input)\n",
        "\n",
        "    total_loss = mse(out * mask_var, img_var * mask_var)\n",
        "    total_loss.backward()\n",
        "    \n",
        "    psrn_masked = compare_psnr(img_masked, out.detach().cpu().numpy()[0] * img_mask_np) \n",
        "    psrn = compare_psnr(img_np, out.detach().cpu().numpy()[0]) \n",
        "\n",
        "    psrns.append(psrn)\n",
        "    psrns_masked.append(psrn_masked)\n",
        "    if psrn > max_psrn:\n",
        "      max_psrn = psrn\n",
        "      max_iter = i\n",
        "    print ('Iteration %05d    Loss %f PSNR_masked %f PSNR %f' % (i, total_loss.item(), psrn_masked, psrn),'\\r', end='')\n",
        "    \n",
        "    \n",
        "    if  PLOT and i % show_every == 0:\n",
        "        out_np = torch_to_np(out)\n",
        "        \n",
        "        # Backtracking\n",
        "        if psrn_masked - psrn_masked_last < -5: \n",
        "            print('Falling back to previous checkpoint.')\n",
        "\n",
        "            for new_param, net_param in zip(last_net, net.parameters()):\n",
        "                net_param.data.copy_(new_param.cuda())\n",
        "\n",
        "            return total_loss*0\n",
        "        else:\n",
        "            last_net = [x.cpu() for x in net.parameters()]\n",
        "            psrn_masked_last = psrn_masked\n",
        "\n",
        "\n",
        "        print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psrn))\n",
        "\n",
        "        plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "  net_input_saved = net_input.detach().clone()\n",
        "  noise = net_input.detach().clone()\n",
        "\n",
        "  # Run\n",
        "  p = get_params(OPT_OVER, net, net_input)\n",
        "  optimize(OPTIMIZER, p, closure, LR=LR, num_iter=num_iter)\n",
        "\n",
        "  print(max_psrn)\n",
        "  print(max_iter)\n",
        "  out_np = torch_to_np(net(net_input))\n",
        "  q = plot_image_grid([np.clip(out_np, 0, 1), img_np], factor=13);\n",
        "\n",
        "  plt.plot(psrns)\n",
        "  plt.xticks(range(0, 11001, 500))\n",
        "  plt.ylabel('psnr')\n",
        "  plt.xlabel('iterations')\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYInO3OO98bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reproduce()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJxiO3JQKcVy",
        "colab_type": "text"
      },
      "source": [
        "### Super Resolution\n",
        "\n",
        "#### Reproduction\n",
        "\n",
        "Another application of deep image priors discussed in the paper is super resolution. The aim of this task is to create a high resolution image from a low resolution image by upsampling it with a factor $t$. In other words, when having a low resolution image $x_0 \\in ℝ^{3 \\times H \\times W}$, you generate the high resolution image $x \\in ℝ^{3 \\times tH \\times tW}$. To solve this problem the data term is set to $E(x;x_0) = ||d(x) - x_0||^2$, where $d(\\cdot) : ℝ^{3 \\times tH \\times tW} \\to ℝ^{3 \\times H \\times W}$ is a downsampling operator that resizes the high resolution image to a low resolution image by a factor $t$. The goal is to minimize $E$ and therefore to find a high resolution image, when downsampling is the same as the original low resolution image. \n",
        "\n",
        "The paper evaluated two datasets and showed the results in the supplementary material. We will reproduce one dataset since we have limited time and the differences in images between the datasets are not very significant in our opinion. It would be more interesting to adjust the code to get better results. The code for reproduction is given below and to run the code, uncomment the line of the image you want to try and the factor of how much you want to upscale the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYbydS1JqmEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path_to_image = 'SR_images/Set5/baby.png'\n",
        "path_to_image = 'SR_images/Set5/bird.png'\n",
        "# path_to_image = 'SR_images/Set5/butterfly.png'\n",
        "# path_to_image = 'SR_images/Set5/head.png'\n",
        "# path_to_image = 'SR_images/Set5/woman.png'\n",
        "\n",
        "factor = 4\n",
        "# factor = 8 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA_v2G7vqKm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from models import *\n",
        "import torch\n",
        "import torch.optim\n",
        "from skimage.measure import compare_psnr\n",
        "from models.downsampler import Downsampler\n",
        "from utils.sr_utils import *\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark =True\n",
        "dtype = torch.cuda.FloatTensor\n",
        "\n",
        "imsize = -1 \n",
        "enforse_div32 = 'CROP' # we usually need the dimensions to be divisible by a power of two (32 in this case)\n",
        "PLOT = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWj7O0SCbIHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starts here\n",
        "imgs = load_LR_HR_imgs_sr(path_to_image , imsize, factor, enforse_div32)\n",
        "\n",
        "\n",
        "input_depth = 32\n",
        " \n",
        "INPUT =     'noise'\n",
        "pad   =     'reflection'\n",
        "OPT_OVER =  'net'\n",
        "KERNEL_TYPE='lanczos2'\n",
        "\n",
        "LR = 0.01\n",
        "tv_weight = 0.0\n",
        "\n",
        "OPTIMIZER = 'adam'\n",
        "\n",
        "if factor == 4: \n",
        "    num_iter = 2000\n",
        "    reg_noise_std = 0.03\n",
        "elif factor == 8:\n",
        "    num_iter = 4000\n",
        "    reg_noise_std = 0.05\n",
        "else:\n",
        "    assert False, 'We did not experiment with other factors'\n",
        "\n",
        "\n",
        "net_input = get_noise(input_depth, INPUT, (imgs['HR_pil'].size[1], imgs['HR_pil'].size[0])).type(dtype).detach()\n",
        "\n",
        "NET_TYPE = 'skip' # UNet, ResNet\n",
        "net = get_net(input_depth, 'skip', pad,\n",
        "              skip_n33d=128, \n",
        "              skip_n33u=128, \n",
        "              skip_n11=4, \n",
        "              num_scales=5,\n",
        "              upsample_mode='bilinear').type(dtype)\n",
        "\n",
        "# Losses\n",
        "mse = torch.nn.MSELoss().type(dtype)\n",
        "\n",
        "img_LR_var = np_to_torch(imgs['LR_np']).type(dtype)\n",
        "\n",
        "downsampler = Downsampler(n_planes=3, factor=factor, kernel_type=KERNEL_TYPE, phase=0.5, preserve_size=True).type(dtype)\n",
        "\n",
        "\n",
        "def closure():\n",
        "    global i, net_input, psnr_best_HR, best_image, best_iteration\n",
        "    \n",
        "    if reg_noise_std > 0:\n",
        "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "\n",
        "    out_HR = net(net_input)\n",
        "    out_LR = downsampler(out_HR)\n",
        "\n",
        "    total_loss = mse(out_LR, img_LR_var) \n",
        "    \n",
        "    if tv_weight > 0:\n",
        "        total_loss += tv_weight * tv_loss(out_HR)\n",
        "        \n",
        "    total_loss.backward()\n",
        "\n",
        "    # Log\n",
        "    psnr_LR = compare_psnr(imgs['LR_np'], torch_to_np(out_LR))\n",
        "    psnr_HR = compare_psnr(imgs['HR_np'], torch_to_np(out_HR))\n",
        "                      \n",
        "    # History\n",
        "    psnr_history.append([psnr_LR, psnr_HR])\n",
        "    \n",
        "    if psnr_HR > psnr_best_HR:\n",
        "        psnr_best_HR = psnr_HR\n",
        "        best_image = out_HR\n",
        "        best_iteration = i\n",
        "\n",
        "    if PLOT and i % 100 == 0:\n",
        "        out_HR_np = torch_to_np(out_HR)\n",
        "        plot_image_grid([imgs['HR_np'], np.clip(out_HR_np, 0, 1)], factor=13, nrow=2)\n",
        "\n",
        "        print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psnr_LR) + \", \" + str(psnr_HR))\n",
        "\n",
        "\n",
        "    i += 1\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "psnr_best_HR = 0\n",
        "best_image = 0\n",
        "best_iteration = 0\n",
        "psnr_history = [] \n",
        "net_input_saved = net_input.detach().clone()\n",
        "noise = net_input.detach().clone()\n",
        "\n",
        "i = 0\n",
        "p = get_params(OPT_OVER, net, net_input)\n",
        "optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
        "\n",
        "\n",
        "out_HR_np = np.clip(torch_to_np(best_image), 0, 1)\n",
        "\n",
        "plot_image_grid([imgs['HR_np'], out_HR_np], factor=13, nrow=2);\n",
        "\n",
        "print(str(best_iteration) + \" : \" + str(psnr_best_HR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8u4QqFp81E",
        "colab_type": "text"
      },
      "source": [
        "The result we got when running all examples are shown in the tables below and compared with the results of Ulyanov et al.\n",
        "\n",
        "\n",
        "| | Baby | Bird | Butterfly | Head | Woman |\n",
        "|-|-|-|-|-|-|\n",
        "| Ulyanov et al. | 31.49 | 31.8 | 26.23 | 31.04 | 28.93 |\n",
        "| Ours LR | 34.36529932402503 | 35.54445532546047 | 37.85091199603584 | 36.5863388389097 | 36.04726591794942 |\n",
        "| Ours HR | 29.533271086683413 | 28.864289695199304 | 24.408632002667886 | 28.205251617471486 | 26.773180053041045 |\n",
        "\n",
        "<div align=\"center\">Super-resoution PSNR comparison on Set5 dataset with factor 4x.</div>\n",
        "\n",
        "| | Baby | Bird | Butterfly | Head | Woman |\n",
        "|-|-|-|-|-|-|\n",
        "| Ulyanov et al. | 28.28 | 27.09 | 20.02 | 29.55 | 24.5 |\n",
        "| Ours LR | 38.900908415152195 | 42.4809241492805 | 42.57606031771982 | 40.95015574831018 | 41.78949489818053 |\n",
        "| Ours HR | 26.954017559788728 | 24.790998174772056 | 18.9492846241358 | 27.07757116122853 | 23.14967179820611 |\n",
        "\n",
        "<div align=\"center\">Super-resoution PSNR comparison on Set5 dataset with factor 8x.</div>\n",
        "\n",
        "\n",
        "#### Hyperparameter Analysis\n",
        "\n",
        "The following code makes it possible to run super resolution with customized parameters and plot graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRKZ2kyopeSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def superResolution(img_path, label, LR = 0.01, factor = 4, num_iter = 2000, reg_noise_std = 0.03):\n",
        "\n",
        "    global i, net_input, psnr_best_HR, best_image, best_iteration\n",
        "\n",
        "\n",
        "    # Starts here\n",
        "    imgs = load_LR_HR_imgs_sr(img_path , imsize, factor, enforse_div32)\n",
        "    plot_image_grid([imgs['HR_np']], factor = 6)\n",
        "    plot_image_grid([imgs['LR_np']], factor = 6)\n",
        "\n",
        "\n",
        "    input_depth = 32\n",
        " \n",
        "    INPUT =     'noise'\n",
        "    pad   =     'reflection'\n",
        "    OPT_OVER =  'net'\n",
        "    KERNEL_TYPE='lanczos2'\n",
        "\n",
        "    tv_weight = 0.0\n",
        "\n",
        "    OPTIMIZER = 'adam'\n",
        "\n",
        "    net_input = get_noise(input_depth, INPUT, (imgs['HR_pil'].size[1], imgs['HR_pil'].size[0])).type(dtype).detach()\n",
        "\n",
        "    NET_TYPE = 'skip' # UNet, ResNet\n",
        "    net = get_net(input_depth, 'skip', pad,\n",
        "                  skip_n33d=128, \n",
        "                  skip_n33u=128, \n",
        "                  skip_n11=4, \n",
        "                  num_scales=5,\n",
        "                  upsample_mode='bilinear').type(dtype)\n",
        "\n",
        "    # Losses\n",
        "    mse = torch.nn.MSELoss().type(dtype)\n",
        "\n",
        "    img_LR_var = np_to_torch(imgs['LR_np']).type(dtype)\n",
        "\n",
        "    downsampler = Downsampler(n_planes=3, factor=factor, kernel_type=KERNEL_TYPE, phase=0.5, preserve_size=True).type(dtype)\n",
        "\n",
        "\n",
        "    def closure():\n",
        "        global i, net_input, psnr_best_HR, best_image, best_iteration\n",
        "    \n",
        "        if reg_noise_std > 0:\n",
        "            net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "\n",
        "        out_HR = net(net_input)\n",
        "        out_LR = downsampler(out_HR)\n",
        "\n",
        "        total_loss = mse(out_LR, img_LR_var) \n",
        "    \n",
        "        if tv_weight > 0:\n",
        "            total_loss += tv_weight * tv_loss(out_HR)\n",
        "        \n",
        "        total_loss.backward()\n",
        "\n",
        "        # Log\n",
        "        psnr_LR = compare_psnr(imgs['LR_np'], torch_to_np(out_LR))\n",
        "        psnr_HR = compare_psnr(imgs['HR_np'], torch_to_np(out_HR))\n",
        "                      \n",
        "        # History\n",
        "        psnr_history.append(psnr_HR)\n",
        "    \n",
        "        if psnr_HR > psnr_best_HR:\n",
        "            psnr_best_HR = psnr_HR\n",
        "            best_image = out_HR\n",
        "            best_iteration = i\n",
        "\n",
        "        if PLOT and i % 100 == 0:\n",
        "            out_HR_np = torch_to_np(out_HR)\n",
        "            plot_image_grid([imgs['HR_np'], np.clip(out_HR_np, 0, 1)], factor=12, nrow=2)\n",
        "\n",
        "            print(str(i) + \"/\" + str(num_iter) + \" : \" + str(psnr_LR) + \", \" + str(psnr_HR))\n",
        "\n",
        "            plt.plot(psnr_history, label = label)\n",
        "            plt.xticks(range(0, num_iter + 1, 500))\n",
        "            plt.ylabel('psnr')\n",
        "            plt.xlabel('iterations')\n",
        "            plt.grid()\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        i += 1\n",
        "    \n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    psnr_best_HR = 0\n",
        "    best_image = 0\n",
        "    best_iteration = 0\n",
        "    psnr_history = [] \n",
        "    net_input_saved = net_input.detach().clone()\n",
        "    noise = net_input.detach().clone()\n",
        "\n",
        "    i = 0\n",
        "    p = get_params(OPT_OVER, net, net_input)\n",
        "    optimize(OPTIMIZER, p, closure, LR, num_iter)\n",
        "\n",
        "\n",
        "    out_HR_np = np.clip(torch_to_np(best_image), 0, 1)\n",
        "\n",
        "    plot_image_grid([imgs['HR_np'], out_HR_np], factor=12, nrow=2);\n",
        "    plot_image_grid([imgs['LR_np']], factor = 6)\n",
        "\n",
        "    print(str(best_iteration) + \" : \" + str(psnr_best_HR))\n",
        "\n",
        "    plt.plot(psnr_history, label = label)\n",
        "    plt.xticks(range(0, num_iter + 1, 500))\n",
        "    plt.ylabel('psnr')\n",
        "    plt.xlabel('iterations')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return psnr_history, label\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCkD7VydqoVy",
        "colab_type": "text"
      },
      "source": [
        "##### Learning rate\n",
        "We first test different learning rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X7ClwNSpfsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1, label1 = superResolution(path_to_image, \"0.1\", LR= 0.1, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
        "data2, label2 = superResolution(path_to_image, \"0.01\", LR= 0.01, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
        "data3, label3 = superResolution(path_to_image, \"0.001\", LR= 0.001, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
        "data4, label4 = superResolution(path_to_image, \"0.0001\", LR= 0.0001, factor = 4, reg_noise_std = 0.03, num_iter = 2000)\n",
        "\n",
        "\n",
        "plt.plot(data1, label = label1)\n",
        "plt.plot(data2, label = label2)\n",
        "plt.plot(data3, label = label3)\n",
        "plt.plot(data4, label = label4)\n",
        "plt.xticks(range(0, 2001, 500))\n",
        "plt.ylabel('psnr')\n",
        "plt.xlabel('iterations')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtSpXlN2c_yi",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deypCvEkLGvq",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions\n",
        "From the experiments run above we can conclude that indeed the results from the original Deep Image Prior paper can be replicated with their code and that the results are very dependend on the choice of hyperparameters. Hyperparameter tweaking is needed when trying this method on a different set of images."
      ]
    }
  ]
}